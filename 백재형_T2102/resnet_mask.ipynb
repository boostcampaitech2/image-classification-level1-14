{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "about-heavy",
   "metadata": {},
   "source": [
    "## 0. Libarary 불러오기 및 경로설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cubic-scoop",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import Resize, ToTensor, Normalize, RandomHorizontalFlip, CenterCrop\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a9620ecb-d78f-42eb-a9ea-291f2dedeff9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/ml/code\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5bcf4900-37b1-40aa-9dbf-ac88573377a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EDA.ipynb  mask.ipynb  models  requirements.txt  sample_submission.ipynb\n"
     ]
    }
   ],
   "source": [
    "!cd /opt/ml/input/data/train/images\n",
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c40701a1-db28-4cd9-8a11-c71be82e740a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "41b7b149-f33e-435e-b06a-79f9ddb43346",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "img_dir = '/opt/ml/input/data/train/images'\n",
    "info_dir = '/opt/ml/input/data/train/train.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b9c4063f-71ef-481b-9df6-16421e9bdada",
   "metadata": {},
   "outputs": [],
   "source": [
    "info = pd.read_csv(info_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5b12d5c7-e817-4de2-ba13-63001ca49b5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>gender</th>\n",
       "      <th>race</th>\n",
       "      <th>age</th>\n",
       "      <th>path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000001</td>\n",
       "      <td>female</td>\n",
       "      <td>Asian</td>\n",
       "      <td>45</td>\n",
       "      <td>000001_female_Asian_45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000002</td>\n",
       "      <td>female</td>\n",
       "      <td>Asian</td>\n",
       "      <td>52</td>\n",
       "      <td>000002_female_Asian_52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000004</td>\n",
       "      <td>male</td>\n",
       "      <td>Asian</td>\n",
       "      <td>54</td>\n",
       "      <td>000004_male_Asian_54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>000005</td>\n",
       "      <td>female</td>\n",
       "      <td>Asian</td>\n",
       "      <td>58</td>\n",
       "      <td>000005_female_Asian_58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>000006</td>\n",
       "      <td>female</td>\n",
       "      <td>Asian</td>\n",
       "      <td>59</td>\n",
       "      <td>000006_female_Asian_59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2695</th>\n",
       "      <td>006954</td>\n",
       "      <td>male</td>\n",
       "      <td>Asian</td>\n",
       "      <td>19</td>\n",
       "      <td>006954_male_Asian_19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2696</th>\n",
       "      <td>006955</td>\n",
       "      <td>male</td>\n",
       "      <td>Asian</td>\n",
       "      <td>19</td>\n",
       "      <td>006955_male_Asian_19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2697</th>\n",
       "      <td>006956</td>\n",
       "      <td>male</td>\n",
       "      <td>Asian</td>\n",
       "      <td>19</td>\n",
       "      <td>006956_male_Asian_19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2698</th>\n",
       "      <td>006957</td>\n",
       "      <td>male</td>\n",
       "      <td>Asian</td>\n",
       "      <td>20</td>\n",
       "      <td>006957_male_Asian_20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2699</th>\n",
       "      <td>006959</td>\n",
       "      <td>male</td>\n",
       "      <td>Asian</td>\n",
       "      <td>19</td>\n",
       "      <td>006959_male_Asian_19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2700 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id  gender   race  age                    path\n",
       "0     000001  female  Asian   45  000001_female_Asian_45\n",
       "1     000002  female  Asian   52  000002_female_Asian_52\n",
       "2     000004    male  Asian   54    000004_male_Asian_54\n",
       "3     000005  female  Asian   58  000005_female_Asian_58\n",
       "4     000006  female  Asian   59  000006_female_Asian_59\n",
       "...      ...     ...    ...  ...                     ...\n",
       "2695  006954    male  Asian   19    006954_male_Asian_19\n",
       "2696  006955    male  Asian   19    006955_male_Asian_19\n",
       "2697  006956    male  Asian   19    006956_male_Asian_19\n",
       "2698  006957    male  Asian   20    006957_male_Asian_20\n",
       "2699  006959    male  Asian   19    006959_male_Asian_19\n",
       "\n",
       "[2700 rows x 5 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "661cc214-a1d6-4ced-ae5b-73bfa01085df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b7c27c1-6a2b-4141-903e-1dd4c2561664",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "de729fcc-351c-482f-aff6-26af2dba9f70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       000001_female_Asian_45\n",
       "1       000002_female_Asian_52\n",
       "2         000004_male_Asian_54\n",
       "3       000005_female_Asian_58\n",
       "4       000006_female_Asian_59\n",
       "                 ...          \n",
       "2695      006954_male_Asian_19\n",
       "2696      006955_male_Asian_19\n",
       "2697      006956_male_Asian_19\n",
       "2698      006957_male_Asian_20\n",
       "2699      006959_male_Asian_19\n",
       "Name: path, Length: 2700, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "person_dir = info['path']\n",
    "person_dir\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "555ad3b8-ec78-4d2f-b897-9e5a7549897e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6ebaafad-1d5d-43fd-b5a3-d4017cd29f6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "female 45\n"
     ]
    }
   ],
   "source": [
    "# split test to get gender and age\n",
    "pdir = person_dir[0]\n",
    "split_str = pdir.split('_')\n",
    "gender = split_str[1]\n",
    "age = split_str[3]\n",
    "\n",
    "print(gender, age)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd9ea5b8-0966-405c-8797-e2d1b38eda44",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "daa15714-a942-4e42-b9d8-3996ff7648b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# numerize classes function\n",
    "\n",
    "def get_class(dict):\n",
    "    if dict['mask'] == 0:#wear\n",
    "        if dict['gen'] == 'male':\n",
    "            if dict['age'] < 30:\n",
    "                return 0\n",
    "            elif 30 <= dict['age'] < 60:\n",
    "                return 1\n",
    "            else:\n",
    "                return 2\n",
    "        else:\n",
    "            if dict['age'] < 30:\n",
    "                return 3\n",
    "            elif 30 <= dict['age'] < 60:\n",
    "                return 4\n",
    "            else:\n",
    "                return 5\n",
    "    elif dict['mask'] == 1:#incorrect\n",
    "        if dict['gen'] == 'male':\n",
    "            if dict['age'] < 30:\n",
    "                return 6\n",
    "            elif 30 <= dict['age'] < 60:\n",
    "                return 7\n",
    "            else:\n",
    "                return 8\n",
    "        else:\n",
    "            if dict['age'] < 30:\n",
    "                return 9\n",
    "            elif 30 <= dict['age'] < 60:\n",
    "                return 10\n",
    "            else:\n",
    "                return 11\n",
    "    elif dict['mask'] == 2:#normal\n",
    "        if dict['gen'] == 'male':\n",
    "            if dict['age'] < 30:\n",
    "                return 12\n",
    "            elif 30 <= dict['age'] < 60:\n",
    "                return 13\n",
    "            else:\n",
    "                return 14\n",
    "        else:\n",
    "            if dict['age'] < 30:\n",
    "                return 15\n",
    "            elif 30 <= dict['age'] < 60:\n",
    "                return 16\n",
    "            else:\n",
    "                return 17\n",
    "\n",
    "get_class({'mask':0, 'gen':'male', 'age':64})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1dcbf15-1e4e-4256-b3c7-44f4a4319f8c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ccbcfebd-1aa4-465a-bf42-33920703d684",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['incorrect_mask', 'normal', 'mask1', 'mask2', 'mask3', 'mask4', 'mask5']\n",
      "18900\n",
      "/opt/ml/input/data/train/images/000001_female_Asian_45/incorrect_mask  ->  10\n",
      "/opt/ml/input/data/train/images/000001_female_Asian_45/normal  ->  16\n",
      "/opt/ml/input/data/train/images/000001_female_Asian_45/mask1  ->  4\n",
      "/opt/ml/input/data/train/images/000001_female_Asian_45/mask2  ->  4\n",
      "/opt/ml/input/data/train/images/000001_female_Asian_45/mask3  ->  4\n",
      "/opt/ml/input/data/train/images/000001_female_Asian_45/mask4  ->  4\n",
      "/opt/ml/input/data/train/images/000001_female_Asian_45/mask5  ->  4\n",
      "/opt/ml/input/data/train/images/000002_female_Asian_52/incorrect_mask  ->  10\n",
      "/opt/ml/input/data/train/images/000002_female_Asian_52/normal  ->  16\n",
      "/opt/ml/input/data/train/images/000002_female_Asian_52/mask1  ->  4\n",
      "/opt/ml/input/data/train/images/000002_female_Asian_52/mask2  ->  4\n",
      "/opt/ml/input/data/train/images/000002_female_Asian_52/mask3  ->  4\n",
      "/opt/ml/input/data/train/images/000002_female_Asian_52/mask4  ->  4\n",
      "/opt/ml/input/data/train/images/000002_female_Asian_52/mask5  ->  4\n",
      "/opt/ml/input/data/train/images/000004_male_Asian_54/incorrect_mask  ->  7\n",
      "/opt/ml/input/data/train/images/000004_male_Asian_54/normal  ->  13\n",
      "/opt/ml/input/data/train/images/000004_male_Asian_54/mask1  ->  1\n",
      "/opt/ml/input/data/train/images/000004_male_Asian_54/mask2  ->  1\n",
      "/opt/ml/input/data/train/images/000004_male_Asian_54/mask3  ->  1\n",
      "/opt/ml/input/data/train/images/000004_male_Asian_54/mask4  ->  1\n",
      "/opt/ml/input/data/train/images/000004_male_Asian_54/mask5  ->  1\n",
      "/opt/ml/input/data/train/images/000005_female_Asian_58/incorrect_mask  ->  10\n",
      "/opt/ml/input/data/train/images/000005_female_Asian_58/normal  ->  16\n",
      "/opt/ml/input/data/train/images/000005_female_Asian_58/mask1  ->  4\n",
      "/opt/ml/input/data/train/images/000005_female_Asian_58/mask2  ->  4\n",
      "/opt/ml/input/data/train/images/000005_female_Asian_58/mask3  ->  4\n",
      "/opt/ml/input/data/train/images/000005_female_Asian_58/mask4  ->  4\n",
      "/opt/ml/input/data/train/images/000005_female_Asian_58/mask5  ->  4\n",
      "/opt/ml/input/data/train/images/000006_female_Asian_59/incorrect_mask  ->  10\n",
      "/opt/ml/input/data/train/images/000006_female_Asian_59/normal  ->  16\n"
     ]
    }
   ],
   "source": [
    "# create image_paths\\\n",
    "# current status: eacp person, each mask or not.\n",
    "# need to sepereate them to a state that (one eg, one label).\n",
    "\n",
    "# may the label process be here? or in dataset? here: two functions in one cell. dataset: time taking and messy index to split.\n",
    "img_paths = []\n",
    "status = ['incorrect_mask', 'normal']\n",
    "\n",
    "labels = []\n",
    "\n",
    "for i in range(5):\n",
    "    status.append('mask' + str(i+1))\n",
    "print(status)\n",
    "\n",
    "for p_dir in person_dir:\n",
    "    \n",
    "    split_str = p_dir.split('_')\n",
    "    gender = split_str[1]\n",
    "    age = split_str[3]\n",
    "    \n",
    "    for idx, st in enumerate(status):\n",
    "        img_paths.append(img_dir + \"/\" + p_dir + \"/\" + st)\n",
    "\n",
    "        label_stat = {}\n",
    "        if idx == 0:\n",
    "            label_stat['mask'] = 1\n",
    "        elif idx == 1:\n",
    "            label_stat['mask'] = 2\n",
    "        else:\n",
    "            label_stat['mask'] = 0\n",
    "        \n",
    "        label_stat['age'] = int(age)\n",
    "        label_stat['gen'] = gender\n",
    "        \n",
    "        labels.append(get_class(label_stat))\n",
    "        \n",
    "        \n",
    "        \n",
    "print(len(img_paths))\n",
    "for i in range(10*3):\n",
    "    print(img_paths[i], \" -> \", labels[i])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21bc4949-cf51-4e90-9abd-b14f6e703423",
   "metadata": {},
   "source": [
    "# create only normal and incorrect dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b0a1306f-16bb-49b8-8a26-2eafe3eafdbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5400\n",
      "/opt/ml/input/data/train/images/000001_female_Asian_45/incorrect_mask  ->  10\n",
      "/opt/ml/input/data/train/images/000001_female_Asian_45/normal  ->  16\n",
      "/opt/ml/input/data/train/images/000002_female_Asian_52/incorrect_mask  ->  10\n",
      "/opt/ml/input/data/train/images/000002_female_Asian_52/normal  ->  16\n",
      "/opt/ml/input/data/train/images/000004_male_Asian_54/incorrect_mask  ->  7\n"
     ]
    }
   ],
   "source": [
    "img_paths_false = []\n",
    "status = ['incorrect_mask', 'normal']\n",
    "\n",
    "labels_false = []\n",
    "\n",
    "# for i in range(5):\n",
    "#     status.append('mask' + str(i+1))\n",
    "# print(status)\n",
    "\n",
    "for p_dir in person_dir:\n",
    "    \n",
    "    split_str = p_dir.split('_')\n",
    "    gender = split_str[1]\n",
    "    age = split_str[3]\n",
    "    \n",
    "    for idx, st in enumerate(status):\n",
    "        img_paths_false.append(img_dir + \"/\" + p_dir + \"/\" + st)\n",
    "\n",
    "        label_stat = {}\n",
    "        if idx == 0:\n",
    "            label_stat['mask'] = 1\n",
    "        elif idx == 1:\n",
    "            label_stat['mask'] = 2\n",
    "        else:\n",
    "            label_stat['mask'] = 0\n",
    "        \n",
    "        label_stat['age'] = int(age)\n",
    "        label_stat['gen'] = gender\n",
    "        \n",
    "        labels_false.append(get_class(label_stat))\n",
    "        \n",
    "        \n",
    "        \n",
    "print(len(img_paths_false))\n",
    "for i in range(5):\n",
    "    print(img_paths_false[i], \" -> \", labels_false[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e7cd20-052b-4c37-8596-cedf02fc33db",
   "metadata": {},
   "source": [
    "# Only positive data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6fb17601-8f0c-4435-a60d-d73a6a8188c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['mask1', 'mask2', 'mask3', 'mask4', 'mask5']\n",
      "13500\n",
      "/opt/ml/input/data/train/images/000001_female_Asian_45/mask1  ->  10\n",
      "/opt/ml/input/data/train/images/000001_female_Asian_45/mask2  ->  16\n",
      "/opt/ml/input/data/train/images/000001_female_Asian_45/mask3  ->  4\n",
      "/opt/ml/input/data/train/images/000001_female_Asian_45/mask4  ->  4\n",
      "/opt/ml/input/data/train/images/000001_female_Asian_45/mask5  ->  4\n"
     ]
    }
   ],
   "source": [
    "img_paths_true = []\n",
    "status = []\n",
    "\n",
    "labels_true = []\n",
    "\n",
    "for i in range(5):\n",
    "    status.append('mask' + str(i+1))\n",
    "print(status)\n",
    "\n",
    "for p_dir in person_dir:\n",
    "    \n",
    "    split_str = p_dir.split('_')\n",
    "    gender = split_str[1]\n",
    "    age = split_str[3]\n",
    "    \n",
    "    for idx, st in enumerate(status):\n",
    "        img_paths_true.append(img_dir + \"/\" + p_dir + \"/\" + st)\n",
    "\n",
    "        label_stat = {}\n",
    "        if idx == 0:\n",
    "            label_stat['mask'] = 1\n",
    "        elif idx == 1:\n",
    "            label_stat['mask'] = 2\n",
    "        else:\n",
    "            label_stat['mask'] = 0\n",
    "        \n",
    "        label_stat['age'] = int(age)\n",
    "        label_stat['gen'] = gender\n",
    "        \n",
    "        labels_true.append(get_class(label_stat))\n",
    "        \n",
    "        \n",
    "        \n",
    "print(len(img_paths_true))\n",
    "for i in range(5):\n",
    "    print(img_paths_true[i], \" -> \", labels_true[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d93a0957-102f-4399-a5a9-a488052123d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskDataset(Dataset):\n",
    "    def __init__(self, img_paths, labels, transform = None):\n",
    "        self.img_paths = img_paths\n",
    "        self.transform = transform\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        pth = self.img_paths[index]\n",
    "        \n",
    "        if os.path.exists(pth + '.jpg'):\n",
    "            self.img_paths[index] = pth + '.jpg'\n",
    "        elif os.path.exists(pth + '.png'):\n",
    "            self.img_paths[index] = pth + '.png'\n",
    "        elif os.path.exists(pth + '.jpeg'):\n",
    "            self.img_paths[index] = pth + '.jpeg'\n",
    "            \n",
    "        image = Image.open(self.img_paths[index])\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        else:\n",
    "#             image = transforms.Resize((64,64))(image)\n",
    "            image = transforms.ToTensor()(image)\n",
    "        return image, self.labels[index]\n",
    "\n",
    "    def show_pic(self, index):\n",
    "        image = Image.open(self.img_paths[index])\n",
    "#         image = transforms.Resize((64,64))(image)\n",
    "        return image\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_paths)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return \"number of dataset : \" + str(self.__len__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2e95bfda-bb43-4603-848a-67654b6bb875",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transforms_img(im):\n",
    "    ######################################TODO######################################\n",
    "    im = transforms.ToTensor()(im)\n",
    "    im = transforms.Resize((224,224))(im)\n",
    "    im = transforms.RandomHorizontalFlip(0.5)(im)\n",
    "    im = transforms.CenterCrop((150,150))(im)\n",
    "\n",
    "    ################################################################################\n",
    "    return im"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26eaee52-bc41-45e1-8aa2-d1a3876d20b4",
   "metadata": {},
   "source": [
    "# Test speed of data generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7426e97e-e420-4970-988e-6bd4f438f6e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # test speed of data generator without transforms\n",
    "# t = MaskDataset(img_paths, labels)\n",
    "# for idx, d in enumerate(tqdm(t)):\n",
    "#     pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c4d5ef16-34c9-4e8e-9961-41a3a914e987",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # test speed of data generator with transforms. totensor, resize, randomhorizontalflip, centercrop\n",
    "# t = MaskDataset(img_paths, labels, get_transforms_img)\n",
    "# for idx, d in enumerate(tqdm(t)):\n",
    "#     pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dbbe3a95-7458-4732-8beb-b4c41e9b84a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # test speed of data generator with transforms. totensor, resize, randomhorizontalflip, centercrop\n",
    "# t = MaskDataset(img_paths, labels)\n",
    "# for idx, d in enumerate(tqdm(t)):\n",
    "#     pass\n",
    "\n",
    "# composed = transforms.Compose([\n",
    "#     ToTensor(),\n",
    "#     Resize((224,224)),\n",
    "#     RandomHorizontalFlip(0.5),\n",
    "#     CenterCrop((150,150))\n",
    "# ])\n",
    "\n",
    "# # test speed of data generator with transforms. totensor, resize, randomhorizontalflip, centercrop\n",
    "# t = MaskDataset(img_paths, labels, composed)\n",
    "# for idx, d in enumerate(tqdm(t)):\n",
    "#     pass\n",
    "\n",
    "# composed = transforms.Compose([\n",
    "\n",
    "#     Resize((224,224)),\n",
    "#     RandomHorizontalFlip(0.5),\n",
    "#     CenterCrop((150,150)),\n",
    "#     ToTensor()\n",
    "# ])\n",
    "\n",
    "# # test speed of data generator with transforms. totensor, resize, randomhorizontalflip, centercrop\n",
    "# t = MaskDataset(img_paths, labels, composed)\n",
    "# for idx, d in enumerate(tqdm(t)):\n",
    "#     pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b6697cb-196a-4754-9753-0a2bc7cd09c2",
   "metadata": {},
   "source": [
    "# Concatinate dataset with correct, incorrect, normal.\n",
    "lets deal with the imbalance!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cb7d2841-0a44-4099-b554-0d1521c90174",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "number of dataset : 18900"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = MaskDataset(img_paths, labels, get_transforms_img)\n",
    "# dataset = MaskDataset(img_paths, labels) # 0.87 0.81 f1\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f4741c86-4846-4ab2-8911-0a9b5cde8ad8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5300\n",
      "100\n",
      "13400\n",
      "100\n",
      "18700\n",
      "200\n"
     ]
    }
   ],
   "source": [
    "# true and false dataset\n",
    "\n",
    "false_dataset = MaskDataset(img_paths_false, labels_false, get_transforms_img)\n",
    "true_dataset = MaskDataset(img_paths_true, labels_true, get_transforms_img)\n",
    "\n",
    "false_len = false_dataset.__len__()\n",
    "true_len = true_dataset.__len__()\n",
    "# int(false_len  * (1/3))\n",
    "\n",
    "\n",
    "false_dataset, false_testset = torch.utils.data.random_split(false_dataset, [ int(false_len) - 100, 100 ], generator=torch.Generator().manual_seed(42))\n",
    "true_dataset, true_testset = torch.utils.data.random_split(true_dataset, [ int(true_len) - 100, 100 ], generator = torch.Generator().manual_seed(42))\n",
    "\n",
    "print(false_dataset.__len__())\n",
    "print(false_testset.__len__())\n",
    "print(true_dataset.__len__())\n",
    "print(true_testset.__len__())\n",
    "\n",
    "concat_dataset = torch.utils.data.ConcatDataset([false_dataset, true_dataset])\n",
    "concat_testset = torch.utils.data.ConcatDataset([false_testset, true_testset])\n",
    "\n",
    "print(len(concat_dataset))\n",
    "print(len(concat_testset))\n",
    "\n",
    "concat_dataloader = torch.utils.data.DataLoader(concat_dataset, batch_size = 64, shuffle = True, num_workers = 2)\n",
    "concat_testloader = torch.utils.data.DataLoader(concat_testset, batch_size = 64, shuffle = True, num_workers = 2)\n",
    "\n",
    "\n",
    "# print(\"false data set len = \", false_dataset.__len__())\n",
    "# print(\"true data set len = \", true_dataset.__len__())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "72499deb-010d-45f4-9f27-d240c0aff21a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset, testset = torch.utils.data.random_split(dataset, [18000,900], generator=torch.Generator().manual_seed(42))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "740bc3f1-ddc7-46ee-9bb6-a1fb4acf93ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18000"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "66eeb8a5-185c-4a3f-af84-c66f41a1f902",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset.show_pic(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3643571b-510b-41c0-be8f-ac4e50ecca55",
   "metadata": {},
   "source": [
    "# issue: there are png, jpeg, pjg multiple extensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "af45d56c-5f56-4ef8-9b6a-03034b5a0986",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>gender</th>\n",
       "      <th>race</th>\n",
       "      <th>age</th>\n",
       "      <th>path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2311</th>\n",
       "      <td>006148</td>\n",
       "      <td>male</td>\n",
       "      <td>Asian</td>\n",
       "      <td>19</td>\n",
       "      <td>006148_male_Asian_19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id gender   race  age                  path\n",
       "2311  006148   male  Asian   19  006148_male_Asian_19"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#png file test\n",
    "info[info['id'] == '006148']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "caa15798-2dc1-46e4-8328-51a6f45ae01d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>gender</th>\n",
       "      <th>race</th>\n",
       "      <th>age</th>\n",
       "      <th>path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2560</th>\n",
       "      <td>006582</td>\n",
       "      <td>female</td>\n",
       "      <td>Asian</td>\n",
       "      <td>20</td>\n",
       "      <td>006582_female_Asian_20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id  gender   race  age                    path\n",
       "2560  006582  female  Asian   20  006582_female_Asian_20"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info[info['id'] == '006582']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "34e07fe8-e20d-495f-b19a-79591cc2b065",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 150, 150])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.__getitem__(2560*7)[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fd6efa1b-f9e0-4ac6-89d0-97453db88906",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 150, 150])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.__getitem__(1)[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ccc1cc7c-db32-42c2-be00-4f6f80b9aaaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[0.8438, 0.8440, 0.8440,  ..., 0.8353, 0.8353, 0.8353],\n",
       "          [0.8478, 0.8490, 0.8490,  ..., 0.8373, 0.8373, 0.8373],\n",
       "          [0.8462, 0.8471, 0.8471,  ..., 0.8392, 0.8392, 0.8392],\n",
       "          ...,\n",
       "          [0.1166, 0.1137, 0.1098,  ..., 0.1473, 0.1482, 0.1467],\n",
       "          [0.1261, 0.1157, 0.1083,  ..., 0.1423, 0.1431, 0.1408],\n",
       "          [0.1120, 0.1052, 0.1082,  ..., 0.1449, 0.1385, 0.1363]],\n",
       " \n",
       "         [[0.8124, 0.8126, 0.8126,  ..., 0.8039, 0.8039, 0.8039],\n",
       "          [0.8164, 0.8176, 0.8176,  ..., 0.8059, 0.8059, 0.8059],\n",
       "          [0.8148, 0.8157, 0.8157,  ..., 0.8078, 0.8078, 0.8078],\n",
       "          ...,\n",
       "          [0.1166, 0.1137, 0.1098,  ..., 0.1473, 0.1482, 0.1467],\n",
       "          [0.1261, 0.1157, 0.1083,  ..., 0.1423, 0.1431, 0.1408],\n",
       "          [0.1120, 0.1052, 0.1082,  ..., 0.1449, 0.1385, 0.1363]],\n",
       " \n",
       "         [[0.7693, 0.7695, 0.7695,  ..., 0.7608, 0.7608, 0.7608],\n",
       "          [0.7732, 0.7745, 0.7745,  ..., 0.7627, 0.7627, 0.7627],\n",
       "          [0.7717, 0.7725, 0.7725,  ..., 0.7647, 0.7647, 0.7647],\n",
       "          ...,\n",
       "          [0.1558, 0.1529, 0.1490,  ..., 0.1866, 0.1874, 0.1859],\n",
       "          [0.1653, 0.1549, 0.1475,  ..., 0.1815, 0.1824, 0.1800],\n",
       "          [0.1512, 0.1444, 0.1474,  ..., 0.1841, 0.1777, 0.1756]]]),\n",
       " 3)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.__getitem__(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8b28acf2-3a6b-46cb-bddd-9c10a8b70bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size = 64, shuffle = True, num_workers = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ea13edba-0d33-473d-aa3f-a850dd6a8e01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 3, 150, 150])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iter(dataloader).next()[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adf29ded-991b-432c-b51f-a63105b4d73b",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "794b3408-f108-4db3-ac77-804d93b76799",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install ipywidgets\n",
    "#!jupyter nbextension enable --py widgetsnbextension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "4d16b094-0f46-481b-87e6-5e49880fc437",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=512, out_features=18, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# resnet test\n",
    "import torchvision\n",
    "mask_resnet_18 = torchvision.models.resnet18(pretrained = True)\n",
    "mask_resnet_18.fc = torch.nn.Linear(512, 18, bias = True)\n",
    "torch.nn.init.xavier_uniform_(mask_resnet_18.fc.weight)\n",
    "mask_resnet_18.fc.bias.data.fill_(0.01)\n",
    "mask_resnet_18\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "3d948f5c-7a5f-4fe4-8022-8a38a061c9ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskResnot(nn.Module):\n",
    "    def __init__(self, num_classes: int = 18):\n",
    "        super(MaskResnot, self).__init__()\n",
    "        self.resnet_18 = mask_resnet_18\n",
    "        \n",
    "        \n",
    "#         self.features = nn.Sequential(\n",
    "#             nn.Conv2d(3, 64, kernel_size=11, stride=4, padding=2),\n",
    "#             nn.BatchNorm2d(64),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#         )\n",
    "#         self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "#         self.classifier = nn.Sequential(\n",
    "#             nn.Dropout(),\n",
    "#             nn.Linear(64, 32),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.Linear(32, num_classes),\n",
    "#         )\n",
    "    def init_params(self):\n",
    "        torch.nn.init.xavier_uniform_(self.resnet_18.fc.weight)\n",
    "        self.resnet_18.fc.bias.data.fill_(0.01)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.resnet_18(x)\n",
    "#         x = self.features(x)\n",
    "#         x = self.avgpool(x)\n",
    "#         x = torch.flatten(x, 1)\n",
    "#         x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "6ecbd925-af53-4e53-b87c-58482c0bc343",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85d25bc0-a7de-4bee-b8fd-f98b3e0f762e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "80d5e113-367a-4a38-a760-01d9fee08334",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MaskResnot(18).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "8c1e9f98-4d26-4cbf-ac90-cc6964158ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "7319a882-76a8-4dae-b034-cf69af04f7bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = nn.CrossEntropyLoss()\n",
    "opt = optim.Adam(model.parameters(), lr = 1e-3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "8a26e448-3900-4b7f-b82a-4fd296178c59",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/293 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch number : 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 293/293 [01:24<00:00,  3.46it/s]\n",
      "  0%|          | 0/293 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18700\n",
      "18700\n",
      "loss :  0.027584284295372786\n",
      "acc:  0.4451871657754011\n",
      "epoch number : 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 293/293 [01:24<00:00,  3.47it/s]\n",
      "  0%|          | 0/293 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18700\n",
      "18700\n",
      "loss :  0.018124582384996875\n",
      "acc:  0.6202673796791444\n",
      "epoch number : 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 293/293 [01:25<00:00,  3.42it/s]\n",
      "  0%|          | 0/293 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18700\n",
      "18700\n",
      "loss :  0.01581354410571848\n",
      "acc:  0.664385026737968\n",
      "epoch number : 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 293/293 [01:25<00:00,  3.43it/s]\n",
      "  0%|          | 0/293 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18700\n",
      "18700\n",
      "loss :  0.014774325113882993\n",
      "acc:  0.683475935828877\n",
      "epoch number : 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 293/293 [01:24<00:00,  3.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18700\n",
      "18700\n",
      "loss :  0.013983054658308387\n",
      "acc:  0.7035294117647058\n",
      "epoch = 5 done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "epochs = 5\n",
    "model.init_params()\n",
    "loss_record = []\n",
    "\n",
    "for p in model.parameters():\n",
    "    p.requires_grad = False\n",
    "for p in model.resnet_18.fc.parameters():\n",
    "# for p in model.fc.parameters():\n",
    "    p.requires_grad = True\n",
    "\n",
    "\n",
    "\n",
    "for e in range(epochs):\n",
    "    print(\"epoch number :\", e)\n",
    "    running_loss = 0\n",
    "    running_acc = 0\n",
    "    n_size = 0\n",
    "    for idx, (batch_in, batch_out) in enumerate(tqdm(concat_dataloader)):\n",
    "        X = batch_in.to(device)\n",
    "        Y = batch_out.to(device)\n",
    "        \n",
    "        n_size += X.size(0)\n",
    "        logits = model(X)\n",
    "        loss_out = loss(logits, Y)\n",
    "        _, preds = torch.max(logits, 1)\n",
    "        \n",
    "        opt.zero_grad()\n",
    "        loss_out.backward()\n",
    "        opt.step()\n",
    "        running_loss += loss_out.item() #* batch_in.size(0)\n",
    "        \n",
    "        running_acc += torch.sum(preds == Y.data).item()\n",
    "        \n",
    "    loss_val_avg = running_loss / len(concat_dataset)\n",
    "    loss_record.append(loss_val_avg)\n",
    "    print(len(concat_dataset))\n",
    "    print(n_size)\n",
    "    print(\"loss : \", loss_val_avg)\n",
    "    print(\"acc: \", running_acc/ n_size)\n",
    "# torch.save(model, './models/'+str(epochs)+ \"_concat_\"+'resnet_18.pt')\n",
    "print(\"epoch = %d done!\" %(epochs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "e4b33951-7c6b-4ebe-a44d-75d64abcf966",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "bfa76a02-02b0-46f1-aa84-3dae69fe640d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "c62795b4-a1f3-4328-92d3-fb193b1c3655",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.027584284295372786,\n",
       " 0.018124582384996875,\n",
       " 0.01581354410571848,\n",
       " 0.014774325113882993,\n",
       " 0.013983054658308387]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "95b1d69d-3b97-426d-a550-281f7872086a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f7f7b787af0>]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(1,1)\n",
    "ax.plot([e+1 for e in range(epochs)], loss_record)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "908d17cb-26c0-468a-862d-12187285f8b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "c1f5c5cb-944d-4d19-9d99-f30b66e2dfd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 102.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy =  0.75\n",
      "mirco average:  0.75\n",
      "marco average:  0.5433088853256921\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "num_acc = 0\n",
    "\n",
    "y_true = []\n",
    "y_pred = []\n",
    "# print(len(false_testset))\n",
    "# print(len(concat_testloader))\n",
    "# for idx, (batch_in, batch_out) in enumerate(tqdm(concat_testloader)):\n",
    "for idx, (batch_in, batch_out) in enumerate(tqdm(DataLoader(false_testset))):\n",
    "    with torch.no_grad():\n",
    "        X = batch_in.to(device)\n",
    "        Y = batch_out.to(device)\n",
    "        \n",
    "        logit = model(X)\n",
    "        _, preds = torch.max(logit, 1)\n",
    "        \n",
    "        num_acc += torch.sum(preds == Y.data)\n",
    "        \n",
    "        y_pred += preds.cpu().detach().numpy().tolist()\n",
    "        y_true += Y.cpu().detach().numpy().tolist()\n",
    "        \n",
    "print(\"accuracy = \", num_acc.item() / len(false_testset))\n",
    "score = f1_score(y_true, y_pred, average = 'micro')\n",
    "print(\"mirco average: \", score)\n",
    "score = f1_score(y_true, y_pred, average = 'macro')\n",
    "print(\"marco average: \", score)\n",
    "# print(y_pred)\n",
    "# print(y_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e48e978c-ecef-4504-907c-0955c26a436f",
   "metadata": {},
   "source": [
    "# leader board\n",
    "without data augmentation(only use ToTensor())\n",
    "* epoch = 1; acc: 68\n",
    "* epoch = 3; acc: 87\n",
    "* epoch = 3; micro f1: 0.87, macro f1:0.81 \n",
    "\n",
    "so far without data agumentation.\n",
    "\n",
    "with data augmentaion,\n",
    "* epoch = 3, micro f1: 86, macro f1:74\n",
    "* epoch = 5. micro f1:91, macro f1:77\n",
    "* epooch = 10. micro 95, micro 93\n",
    "* epoch = 20, micro 96, macro 94\n",
    "* epoch = 40, micro 98, macro 96\n",
    "\n",
    "\n",
    "concat dataset\n",
    "* epoch = 3, micro 83, macro 82\n",
    "* epoch = 5, micro 90, macro 91\n",
    "* epoch = 20, micro 95, macro 93\n",
    "* epoch = 30, micro 95, macro 95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "built-elevation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트 데이터셋 폴더 경로를 지정해주세요.\n",
    "test_dir = '/opt/ml/input/data/eval'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quiet-organizer",
   "metadata": {},
   "source": [
    "## 1. Model 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "acknowledged-easter",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(nn.Module):\n",
    "    def __init__(self, num_classes: int = 1000):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=11, stride=4, padding=2),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(32, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.features(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "domestic-channels",
   "metadata": {},
   "source": [
    "## 2. Test Dataset 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "extensive-north",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestDataset(Dataset):\n",
    "    def __init__(self, img_paths, transform):\n",
    "        self.img_paths = img_paths\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image = Image.open(self.img_paths[index])\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "continued-feelings",
   "metadata": {},
   "source": [
    "## 3. Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "coral-shade",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test inference is done!\n"
     ]
    }
   ],
   "source": [
    "# meta 데이터와 이미지 경로를 불러옵니다.\n",
    "submission = pd.read_csv(os.path.join(test_dir, 'info.csv'))\n",
    "image_dir = os.path.join(test_dir, 'images')\n",
    "\n",
    "# Test Dataset 클래스 객체를 생성하고 DataLoader를 만듭니다.\n",
    "image_paths = [os.path.join(image_dir, img_id) for img_id in submission.ImageID]\n",
    "transform = transforms.Compose([\n",
    "    Resize((512, 384), Image.BILINEAR),\n",
    "    ToTensor(),\n",
    "    Normalize(mean=(0.5, 0.5, 0.5), std=(0.2, 0.2, 0.2)),\n",
    "])\n",
    "dataset = TestDataset(image_paths, transform)\n",
    "\n",
    "loader = DataLoader(\n",
    "    dataset,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# 모델을 정의합니다. (학습한 모델이 있다면 torch.load로 모델을 불러주세요!)\n",
    "device = torch.device('cuda')\n",
    "# model = MyModel(num_classes=18).to(device)\n",
    "model.eval()\n",
    "\n",
    "# 모델이 테스트 데이터셋을 예측하고 결과를 저장합니다.\n",
    "all_predictions = []\n",
    "for images in loader:\n",
    "    with torch.no_grad():\n",
    "        images = images.to(device)\n",
    "        pred = model(images)\n",
    "        pred = pred.argmax(dim=-1)\n",
    "        all_predictions.extend(pred.cpu().numpy())\n",
    "submission['ans'] = all_predictions\n",
    "\n",
    "# 제출할 파일을 저장합니다.\n",
    "submission.to_csv(os.path.join(test_dir, 'submission.csv'), index=False)\n",
    "print('test inference is done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "verbal-sample",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
