{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ac6df1d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /opt/conda/lib/python3.8/site-packages (1.7.1)\n",
      "Requirement already satisfied: typing_extensions in /opt/conda/lib/python3.8/site-packages (from torch) (3.7.4.3)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.8/site-packages (from torch) (1.19.2)\n",
      "Requirement already satisfied: torchvision in /opt/conda/lib/python3.8/site-packages (0.8.2)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.8/site-packages (from torchvision) (1.19.2)\n",
      "Requirement already satisfied: torch in /opt/conda/lib/python3.8/site-packages (from torchvision) (1.7.1)\n",
      "Requirement already satisfied: pillow>=4.1.1 in /opt/conda/lib/python3.8/site-packages (from torchvision) (8.1.0)\n",
      "Requirement already satisfied: typing_extensions in /opt/conda/lib/python3.8/site-packages (from torch->torchvision) (3.7.4.3)\n",
      "Requirement already satisfied: timm in /opt/conda/lib/python3.8/site-packages (0.4.12)\n",
      "Requirement already satisfied: torch>=1.4 in /opt/conda/lib/python3.8/site-packages (from timm) (1.7.1)\n",
      "Requirement already satisfied: torchvision in /opt/conda/lib/python3.8/site-packages (from timm) (0.8.2)\n",
      "Requirement already satisfied: typing_extensions in /opt/conda/lib/python3.8/site-packages (from torch>=1.4->timm) (3.7.4.3)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.8/site-packages (from torch>=1.4->timm) (1.19.2)\n",
      "Requirement already satisfied: pillow>=4.1.1 in /opt/conda/lib/python3.8/site-packages (from torchvision->timm) (8.1.0)\n",
      "Requirement already satisfied: wandb in /opt/conda/lib/python3.8/site-packages (0.12.1)\n",
      "Requirement already satisfied: PyYAML in /opt/conda/lib/python3.8/site-packages (from wandb) (5.3.1)\n",
      "Requirement already satisfied: sentry-sdk>=1.0.0 in /opt/conda/lib/python3.8/site-packages (from wandb) (1.3.1)\n",
      "Requirement already satisfied: protobuf>=3.12.0 in /opt/conda/lib/python3.8/site-packages (from wandb) (3.17.3)\n",
      "Requirement already satisfied: pathtools in /opt/conda/lib/python3.8/site-packages (from wandb) (0.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /opt/conda/lib/python3.8/site-packages (from wandb) (2.8.2)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /opt/conda/lib/python3.8/site-packages (from wandb) (2.24.0)\n",
      "Requirement already satisfied: subprocess32>=3.5.3 in /opt/conda/lib/python3.8/site-packages (from wandb) (3.5.4)\n",
      "Requirement already satisfied: GitPython>=1.0.0 in /opt/conda/lib/python3.8/site-packages (from wandb) (3.1.18)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /opt/conda/lib/python3.8/site-packages (from wandb) (0.4.0)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /opt/conda/lib/python3.8/site-packages (from wandb) (5.7.2)\n",
      "Requirement already satisfied: shortuuid>=0.5.0 in /opt/conda/lib/python3.8/site-packages (from wandb) (1.0.1)\n",
      "Requirement already satisfied: promise<3,>=2.0 in /opt/conda/lib/python3.8/site-packages (from wandb) (2.3)\n",
      "Requirement already satisfied: six>=1.13.0 in /opt/conda/lib/python3.8/site-packages (from wandb) (1.15.0)\n",
      "Requirement already satisfied: configparser>=3.8.1 in /opt/conda/lib/python3.8/site-packages (from wandb) (5.0.2)\n",
      "Requirement already satisfied: Click!=8.0.0,>=7.0 in /opt/conda/lib/python3.8/site-packages (from wandb) (8.0.1)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.8/site-packages (from sentry-sdk>=1.0.0->wandb) (2020.12.5)\n",
      "Requirement already satisfied: urllib3>=1.10.0 in /opt/conda/lib/python3.8/site-packages (from sentry-sdk>=1.0.0->wandb) (1.25.11)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.8/site-packages (from requests<3,>=2.0.0->wandb) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /opt/conda/lib/python3.8/site-packages (from requests<3,>=2.0.0->wandb) (3.0.4)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /opt/conda/lib/python3.8/site-packages (from GitPython>=1.0.0->wandb) (4.0.7)\n",
      "Requirement already satisfied: smmap<5,>=3.0.1 in /opt/conda/lib/python3.8/site-packages (from gitdb<5,>=4.0.1->GitPython>=1.0.0->wandb) (4.0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mtkdlqh2\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pip install torch\n",
    "!pip install torchvision\n",
    "!pip install timm\n",
    "!pip install wandb\n",
    "\n",
    "import os\n",
    "import sys\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "from PIL import Image\n",
    "from tqdm.notebook import tqdm\n",
    "from time import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import multiprocessing as mp\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.utils.data as data\n",
    "\n",
    "from torchvision import transforms, models\n",
    "from torchvision.transforms import Resize, ToTensor, Normalize\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import csv\n",
    "import timm\n",
    "import wandb\n",
    "\n",
    "\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "57be33d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: [cuda:0]\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"device: [%s]\"%device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "15106347",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = '/opt/ml/input/data/train'\n",
    "df = pd.read_csv(os.path.join(train_dir, 'train.csv'))\n",
    "image_dir = os.path.join(train_dir, 'images')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "db364832",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = (0.548, 0.504, 0.479)\n",
    "std = (0.237, 0.247, 0.246)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9fd3b84f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random seed 설정\n",
    "SEED = 2021\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)  # type: ignore\n",
    "torch.backends.cudnn.deterministic = True  # type: ignore\n",
    "torch.backends.cudnn.benchmark = True  # type: ignore\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b0c752c",
   "metadata": {},
   "source": [
    "# Labeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0d99d01c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 잘못된 정보를 수정한 새로운 csv파일 생성, path는 그대로\n",
    "\n",
    "def get_fixed_labeled_csv(): \n",
    "    df = pd.read_csv(f\"{train_data_path}/train.csv\")\n",
    "\n",
    "    id_overlap_error = [\"003397\"]\n",
    "    gender_labeling_error = ['006359', '006360', '006361', '006362', '006363', '006364']\n",
    "    mask_labeling_error = ['000020', '004418', '005227']\n",
    "\n",
    "    id_max = int(max(df['id']))\n",
    "    id_new = id_max+1\n",
    "\n",
    "    new_data_list=[]\n",
    "\n",
    "    for idx in tqdm(range(len(df))):  # tqdm 을 이용하면 현재 데이터가 얼마나 처리되고 있는지 파악되어 좋습니다.\n",
    "        _path = df['path'].iloc[idx]  # 순서대로 가져와야 하기 때문에 iloc을 사용해 가져옵니다.\n",
    "        _gender = df['gender'].iloc[idx]\n",
    "        _age = df['age'].iloc[idx]\n",
    "        _id = df['id'].iloc[idx]\n",
    "\n",
    "        if _id in id_overlap_error:\n",
    "            _id='%06d'%(id_new)\n",
    "            id_new += 1\n",
    "        \n",
    "        if _id in gender_labeling_error:\n",
    "            if _gender == \"male\":\n",
    "                _gender = 'female'\n",
    "            else:\n",
    "                _gender = 'male'\n",
    "        \n",
    "        for img_name in Path(f\"{train_image_path}/{_path}\").iterdir():  # 각 dir의 이미지들을 iterative 하게 가져옵니다.\n",
    "            img_stem = img_name.stem  # 해당 파일의 파일명만을 가져옵니다. 확장자 제외.\n",
    "            if not img_stem.startswith('._'):  # avoid hidden files\n",
    "                if _id in mask_labeling_error:\n",
    "                    if img_stem == \"incorrect_mask\":\n",
    "                        img_stem = 'normal'\n",
    "                    elif img_stem == 'normal':\n",
    "                        img_stem = 'incorrect_mask'\n",
    "                new_data_list.append([_id, _age, _gender, img_stem, img_name.__str__()]) \n",
    "        \n",
    "    df = pd.DataFrame(new_data_list)\n",
    "    df.columns = ['id', 'age', 'gender', 'stem', 'img_path']\n",
    "    \n",
    "    df['label'] = 0  # SET SCORE\n",
    "    # AGE\n",
    "    df['label'] += ((df['age'] >= 30) & (df['age'] < 60))*1\n",
    "    df['label'] += (df['age'] >= 60)*2\n",
    "\n",
    "    # GENDER\n",
    "    df['label'] += (df['gender'] == 'female')*3\n",
    "\n",
    "    # MASK wearing condition\n",
    "    df['label'] += (df['stem'].isin(['incorrect_mask']))*6\n",
    "    df['label'] += (df['stem'].isin(['normal']))*12\n",
    "\n",
    "    df.to_csv('./labeled_data.csv', sep=',' ,na_rep='NaN')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcf153c5",
   "metadata": {},
   "source": [
    "라벨링을 fix한 결과를 labeled_data.csv 로 저장"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4db81683",
   "metadata": {},
   "source": [
    "dataset에 넣기 위해 label 과 image_path를 마스크 쓴 상태에 따라 분류하고 이를 분류한 이유는 class imbalance를 어느정도 해소하기 위해 마스크를 안 쓴 데이터와 마스크를 제대로 쓰지 않은 사진만 argumentation으로 부풀리기 위해서 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "34c10331",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"labeled_data.csv\")\n",
    "\n",
    "labels_normal = df[df['stem'] == 'normal'].label\n",
    "labels_incorrect = df[df['stem'] == 'incorrect_mask'].label\n",
    "labels_masked = df[df['stem'].str.contains(\"mask\")].label\n",
    "\n",
    "image_path_normal = df[df['stem'] == 'normal'].img_path\n",
    "image_path_incorrect = df[df['stem'] == 'incorrect_mask'].img_path\n",
    "image_path_masked = df[df['stem'].str.contains(\"mask\")].img_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8fd8912",
   "metadata": {},
   "source": [
    "### 2.1.2 Albumentation Style Augmentation Function\n",
    "- Albumentation은 numpy 형식으로 이미지를 받아 데이터를 변형시킵니다.\n",
    "- opencv 기반으로 빠르고, 다양한 Augmentation 방법이 제공되는 점에서 장점이 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f6bb75ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from albumentations import *\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "\n",
    "def get_transforms(need=('train1','train2','train3', 'val'), img_size=(512, 384), mean=(0.548, 0.504, 0.479), std=(0.237, 0.247, 0.246)):\n",
    "    \"\"\"\n",
    "    train 혹은 validation의 augmentation 함수를 정의합니다. train은 데이터에 많은 변형을 주어야하지만, validation에는 최소한의 전처리만 주어져야합니다.\n",
    "    \n",
    "    Args:\n",
    "        need: 'train', 혹은 'val' 혹은 둘 다에 대한 augmentation 함수를 얻을 건지에 대한 옵션입니다.\n",
    "        img_size: Augmentation 이후 얻을 이미지 사이즈입니다.\n",
    "        mean: 이미지를 Normalize할 때 사용될 RGB 평균값입니다.\n",
    "        std: 이미지를 Normalize할 때 사용될 RGB 표준편차입니다.\n",
    "\n",
    "    Returns:\n",
    "        transformations: Augmentation 함수들이 저장된 dictionary 입니다. transformations['train']은 train 데이터에 대한 augmentation 함수가 있습니다.\n",
    "    \"\"\"\n",
    "    transformations = {}\n",
    "    if 'train1' in need:\n",
    "        transformations['train1'] = Compose([\n",
    "            Resize(img_size[0], img_size[1], p=1.0),\n",
    "            HorizontalFlip(p=1),\n",
    "            Normalize(mean=mean, std=std, max_pixel_value=255.0, p=1.0),\n",
    "            ToTensorV2(p=1.0),\n",
    "        ], p=1.0)\n",
    "\n",
    "    if 'train2' in need:\n",
    "        transformations['train2'] = Compose([\n",
    "            Resize(img_size[0], img_size[1], p=1.0),\n",
    "            RandomBrightnessContrast(brightness_limit=(-0.1, 0.1), contrast_limit=(-0.1, 0.1), p=1),\n",
    "            Normalize(mean=mean, std=std, max_pixel_value=255.0, p=1.0),\n",
    "            ToTensorV2(p=1.0),\n",
    "        ], p=1.0)\n",
    "    \n",
    "    if 'train3' in need:\n",
    "        transformations['train3'] = Compose([\n",
    "            Resize(img_size[0], img_size[1], p=1.0),\n",
    "            GaussNoise(p=0.1),\n",
    "            Normalize(mean=mean, std=std, max_pixel_value=255.0, p=1.0),\n",
    "            ToTensorV2(p=1.0),\n",
    "        ], p=1.0)\n",
    "\n",
    "    if 'val' in need:\n",
    "        transformations['val'] = Compose([\n",
    "            Resize(img_size[0], img_size[1]),\n",
    "            Normalize(mean=mean, std=std, max_pixel_value=255.0, p=1.0),\n",
    "            ToTensorV2(p=1.0),\n",
    "        ], p=1.0)\n",
    "    return transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce67bffd",
   "metadata": {},
   "source": [
    "# Train Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d0e8a094",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskBaseDataset(data.Dataset):\n",
    "\n",
    "    def __init__(self,image_path,labels,transform=None):\n",
    "        self.image_path = image_path\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "        \n",
    "\n",
    "    def set_transform(self, transform):\n",
    "        \"\"\"\n",
    "        transform 함수를 설정하는 함수입니다.\n",
    "        \"\"\"\n",
    "        self.transform = transform\n",
    "        \n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        데이터를 불러오는 함수입니다. \n",
    "        데이터셋 class에 데이터 정보가 저장되어 있고, index를 통해 해당 위치에 있는 데이터 정보를 불러옵니다.\n",
    "        \n",
    "        Args:\n",
    "            index: 불러올 데이터의 인덱스값입니다.\n",
    "        \"\"\"\n",
    "        # 이미지를 불러옵니다.\n",
    "        image = Image.open(image_path[index])\n",
    "        \n",
    "        # 이미지를 Augmentation 시킵니다.\n",
    "        image_transform = self.transform(image=np.array(image))['image']\n",
    "        return image_transform, self.label[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "fc11eaa5",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-44-2f43aae88475>, line 51)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-44-2f43aae88475>\"\u001b[0;36m, line \u001b[0;32m51\u001b[0m\n\u001b[0;31m    train_datasets = torch.utils.data.ConcatDataset([\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# 정의한 Augmentation 함수와 Dataset 클래스 객체를 생성합니다.\n",
    "transform = get_transforms(mean=mean, std=std)\n",
    "\n",
    "dataset_normal = MaskBaseDataset(image_path_normal,labels_normal, transform)\n",
    "dataset_incorrect = MaskBaseDataset(image_path_incorrect,labels_incorrect, transform)\n",
    "dataset_masked = MaskBaseDataset(image_path_masked,labels_masked, transform)\n",
    "\n",
    "# train dataset과 validation dataset을 8:2 비율로 나눕니다.\n",
    "n_val = int(len(dataset_normal) * 0.2)\n",
    "n_train = len(dataset_normal) - n_val\n",
    "train_dataset_normal, val_dataset_normal = data.random_split(dataset_normal, [n_train, n_val])\n",
    "\n",
    "n_val = int(len(dataset_incorrect) * 0.2)\n",
    "n_train = len(dataset_incorrect) - n_val\n",
    "train_dataset_incorrect, val_dataset_incorrect = data.random_split(dataset_incorrect, [n_train, n_val])\n",
    "\n",
    "\n",
    "n_val = int(len(dataset_masked) * 0.2)\n",
    "n_train = len(dataset_masked) - n_val\n",
    "train_dataset_masked, val_dataset_masked = data.random_split(dataset_masked, [n_train, n_val])\n",
    "\n",
    "train_dataset_normal_1=train_dataset_normal\n",
    "train_dataset_normal_2=train_dataset_normal\n",
    "train_dataset_normal_3=train_dataset_normal\n",
    "\n",
    "\n",
    "train_dataset_incorrect_1=train_dataset_incorrect\n",
    "train_dataset_incorrect_2=train_dataset_incorrect\n",
    "train_dataset_incorrect_3=train_dataset_incorrect\n",
    "\n",
    "\n",
    "val_dataset = torch.utils.data.ChainDataset([\n",
    "    val_dataset_normal,val_dataset_incorrect,val_dataset_masked\n",
    "        ])\n",
    "\n",
    "# 각 dataset에 augmentation 함수를 설정합니다.\n",
    "train_dataset_masked.dataset.set_transform(transform['val'])\n",
    "val_dataset_masked.dataset.set_transform(transform['val'])\n",
    "\n",
    "train_dataset_normal_1.dataset.set_transform(transform['train1'])\n",
    "train_dataset_normal_2.dataset.set_transform(transform['train2'])\n",
    "train_dataset_normal_3.dataset.set_transform(transform['train3'])\n",
    "val_dataset_normal.dataset.set_transform(transform['val'])\n",
    "\n",
    "train_dataset_incorrect_1.dataset.set_transform(transform['train1'])\n",
    "train_dataset_incorrect_2.dataset.set_transform(transform['train2'])\n",
    "train_dataset_incorrect_3.dataset.set_transform(transform['train3'])\n",
    "val_dataset_incorrect.dataset.set_transform(transform['val']\n",
    "\n",
    "\n",
    "train_datasets = torch.utils.data.ConcatDataset([\n",
    "    train_dataset_normal_1,train_dataset_normal_2,train_dataset_normal_3,\n",
    "    train_dataset_incorrect_1,train_dataset_incorrect_2,train_dataset_incorrect_3,train_dataset_masked \n",
    "                                          ])\n",
    "\n",
    "val_datasets = torch.utils.data.ConcatDataset([\n",
    "    val_dataset_normal,val_dataset_incorrect,val_dataset_masked\n",
    "                                          ])\n",
    "                                            \n",
    "image_datasets={'train':train_datasets,'validation':val_datasets}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a0d4b9",
   "metadata": {},
   "source": [
    "# DataLoader\n",
    "- 정의한 Dataset을 바탕으로 DataLoader을 생성합니다.\n",
    "- Dataset은 이미지 한장을 주는 모듈이라면, DataLoader은 여러 이미지를 batch_size만큼 묶어 전달해줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7b6ee035",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training dataloader은 데이터를 섞어주어야 합니다. (shuffle=True)\n",
    "dataloaders = {\n",
    "    'train':\n",
    "    data.DataLoader(image_datasets['train'],\n",
    "                    batch_size=12,\n",
    "                    shuffle=True,\n",
    "                    num_workers=4),  # for Kaggle\n",
    "    'validation':\n",
    "    data.DataLoader(image_datasets['validation'],\n",
    "                    batch_size=12,\n",
    "                    shuffle=False,\n",
    "                    num_workers=4)  # for Kaggle\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6077d1fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images shape: torch.Size([12, 3, 512, 384])\n",
      "labels shape: torch.Size([12])\n"
     ]
    }
   ],
   "source": [
    "images, labels = next(iter(dataloaders['train']))\n",
    "print(f'images shape: {images.shape}')\n",
    "print(f'labels shape: {labels.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e6af84",
   "metadata": {},
   "source": [
    "# Model 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d047fccf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/tf_efficientnet_l2_ns-df73bb44.pth\" to /opt/ml/.cache/torch/hub/checkpoints/tf_efficientnet_l2_ns-df73bb44.pth\n"
     ]
    }
   ],
   "source": [
    "model = timm.create_model('tf_efficientnet_l2_ns', pretrained = True)\n",
    "    \n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False   \n",
    "    \n",
    "model.classifier = nn.Linear(in_features=5504, out_features = 18, bias = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "aff4a1ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = df['label'].value_counts()\n",
    "counts_fixed = []\n",
    "for class_number in range(18):\n",
    "    if class_number <6:\n",
    "        counts_fixed.append(counts[class_numeber])\n",
    "    else:\n",
    "        number_of_augments = 3\n",
    "        counts_fixed.append(number_of_augments*counts[class_numeber])\n",
    "\n",
    "counts_normalized = torch.Tensor(counts_fixed / sum(counts_fixed)) \n",
    "weights = 1.0/counts_normalized\n",
    "weights = weights / sum(weights)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(counts_normalized)\n",
    "optimizer = optim.Adam(model.classifier.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "536aef96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, num_epochs=5):\n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        epoch_f1 = 0\n",
    "        print('Epoch {}/{}'.format(epoch+1, num_epochs))\n",
    "        print('-' * 10)\n",
    "        \n",
    "        for phase in ['train', 'validation']:\n",
    "            if phase == 'train':\n",
    "                model.train()\n",
    "            else:\n",
    "                model.eval()\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                if phase == 'train':\n",
    "                    optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            epoch_loss = running_loss / len(image_datasets[phase])\n",
    "            epoch_acc = running_corrects.double() / len(image_datasets[phase])\n",
    "\n",
    "            print('{} loss: {:.4f}, acc: {:.4f}'.format(phase,\n",
    "                                                        epoch_loss,\n",
    "                                                        epoch_acc))\n",
    "            wandb.log({\n",
    "        \"Epoch_Accuracy\": epoch_acc,\n",
    "        \"Test Loss\": epoch_loss})\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "386f572d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db6335aab69b4fa9aafd903075566563",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=30.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "----------\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'dataloaders' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-73fb7330be80>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel_trained\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-45-285d8595f01e>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, criterion, optimizer, num_epochs)\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0mrunning_corrects\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataloaders\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mphase\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m                 \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m                 \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dataloaders' is not defined"
     ]
    }
   ],
   "source": [
    "model_trained = train_model(model, criterion, optimizer, num_epochs=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "785e953a",
   "metadata": {},
   "source": [
    "## Save and load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "daadd5a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory 'models': File exists\n",
      "mkdir: cannot create directory 'models/pytorch': File exists\n"
     ]
    }
   ],
   "source": [
    "!mkdir models\n",
    "!mkdir models/pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8a9f00a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model_trained, 'models/pytorch/efficientNet.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faa4c0a1",
   "metadata": {},
   "source": [
    "## 3. Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "aba78e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # meta 데이터와 이미지 경로를 불러옵니다.\n",
    "test_dir = '/opt/ml/input/data/eval'\n",
    "submission = pd.read_csv(os.path.join(test_dir, 'info.csv'))\n",
    "test_image_dir = os.path.join(test_dir, 'images')\n",
    "\n",
    "# # Test Dataset 클래스 객체를 생성하고 DataLoader를 만듭니다.\n",
    "test_image_paths = [os.path.join(test_image_dir, img_id) for img_id in submission.ImageID]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "33544ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_transforms(img_size=(512, 384), mean=(0.548, 0.504, 0.479), std=(0.237, 0.247, 0.246)):\n",
    "    transformations = Compose([\n",
    "                Resize(img_size[0], img_size[1]),\n",
    "                Normalize(mean=mean, std=std, max_pixel_value=255.0, p=1.0),\n",
    "                ToTensorV2(p=1.0),\n",
    "            ], p=1.0)\n",
    "    return transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cba6315a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_transform = get_test_transforms(mean=mean, std=std)\n",
    "\n",
    "test_dataset = TestDataset(test_image_paths, test_transform)\n",
    "\n",
    "test_loader = data.DataLoader(\n",
    "    test_dataset,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d800eb99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62676e5b633a44dc9dc607daf7343ffa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=12600.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "test inference is done!\n"
     ]
    }
   ],
   "source": [
    "# 모델을 정의합니다. (학습한 모델이 있다면 torch.load로 모델을 불러주세요!)\n",
    "model = torch.load('models/pytorch/weights.h5')\n",
    "model.eval()\n",
    "\n",
    "# 모델이 테스트 데이터셋을 예측하고 결과를 저장합니다.\n",
    "all_predictions = []\n",
    "for images in tqdm(test_loader):\n",
    "    with torch.no_grad():\n",
    "        images = images.to(device)\n",
    "        pred = model(images)\n",
    "        pred = pred.argmax(dim=-1)\n",
    "        all_predictions.extend(pred.cpu().numpy())\n",
    "submission['ans'] = all_predictions\n",
    "\n",
    "# 제출할 파일을 저장합니다.\n",
    "submission.to_csv(os.path.join(test_dir, 'submission.csv'), index=False)\n",
    "print('test inference is done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97438f05",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
