{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e7dcc890-b745-4ef0-85aa-d55ed6dc7811",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.8/site-packages (1.1.5)\n",
      "Requirement already satisfied: pytz>=2017.2 in /opt/conda/lib/python3.8/site-packages (from pandas) (2020.5)\n",
      "Requirement already satisfied: numpy>=1.15.4 in /opt/conda/lib/python3.8/site-packages (from pandas) (1.19.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.8/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.8/site-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n",
      "Hit:1 http://archive.ubuntu.com/ubuntu bionic InRelease                        \n",
      "Hit:2 http://security.ubuntu.com/ubuntu bionic-security InRelease              \n",
      "Get:3 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
      "Get:4 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n",
      "Fetched 163 kB in 2s (72.8 kB/s)  \n",
      "Reading package lists... Done\n",
      "Reading package lists... Done\n",
      "Building dependency tree       \n",
      "Reading state information... Done\n",
      "libgl1-mesa-glx is already the newest version (20.0.8-0ubuntu1~18.04.1).\n",
      "0 upgraded, 0 newly installed, 0 to remove and 24 not upgraded.\n",
      "Requirement already satisfied: opencv-python in /opt/conda/lib/python3.8/site-packages (4.5.1.48)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /opt/conda/lib/python3.8/site-packages (from opencv-python) (1.19.2)\n",
      "Collecting albumentations\n",
      "  Downloading albumentations-1.0.3-py3-none-any.whl (98 kB)\n",
      "\u001b[K     |████████████████████████████████| 98 kB 1.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting opencv-python-headless>=4.1.1\n",
      "  Downloading opencv_python_headless-4.5.3.56-cp38-cp38-manylinux2014_x86_64.whl (37.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 37.1 MB 6.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting scikit-image>=0.16.1\n",
      "  Downloading scikit_image-0.18.3-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.whl (30.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 30.2 MB 31.9 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: PyYAML in /opt/conda/lib/python3.8/site-packages (from albumentations) (5.3.1)\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.8/site-packages (from albumentations) (1.7.1)\n",
      "Requirement already satisfied: numpy>=1.11.1 in /opt/conda/lib/python3.8/site-packages (from albumentations) (1.19.2)\n",
      "Collecting PyWavelets>=1.1.1\n",
      "  Downloading PyWavelets-1.1.1-cp38-cp38-manylinux1_x86_64.whl (4.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 4.4 MB 72.5 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: matplotlib!=3.0.0,>=2.0.0 in /opt/conda/lib/python3.8/site-packages (from scikit-image>=0.16.1->albumentations) (3.2.1)\n",
      "Collecting networkx>=2.0\n",
      "  Downloading networkx-2.6.2-py3-none-any.whl (1.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.9 MB 73.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting imageio>=2.3.0\n",
      "  Downloading imageio-2.9.0-py3-none-any.whl (3.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.3 MB 69.1 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pillow!=7.1.0,!=7.1.1,>=4.3.0 in /opt/conda/lib/python3.8/site-packages (from scikit-image>=0.16.1->albumentations) (8.1.0)\n",
      "Collecting tifffile>=2019.7.26\n",
      "  Downloading tifffile-2021.8.8-py3-none-any.whl (171 kB)\n",
      "\u001b[K     |████████████████████████████████| 171 kB 81.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /opt/conda/lib/python3.8/site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.16.1->albumentations) (2.4.7)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.8/site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.16.1->albumentations) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.8/site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.16.1->albumentations) (0.10.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /opt/conda/lib/python3.8/site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.16.1->albumentations) (2.8.2)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.8/site-packages (from cycler>=0.10->matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.16.1->albumentations) (1.15.0)\n",
      "Installing collected packages: opencv-python-headless, PyWavelets, networkx, imageio, tifffile, scikit-image, albumentations\n",
      "Successfully installed PyWavelets-1.1.1 albumentations-1.0.3 imageio-2.9.0 networkx-2.6.2 opencv-python-headless-4.5.3.56 scikit-image-0.18.3 tifffile-2021.8.8\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas\n",
    "!apt-get update\n",
    "!apt-get -y install libgl1-mesa-glx\n",
    "!pip install opencv-python\n",
    "!pip install albumentations\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import cv2\n",
    "import csv\n",
    "import torch\n",
    "import torch.utils.data as data\n",
    "\n",
    "from albumentations import *\n",
    "from albumentations.pytorch import ToTensorV2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1cfce9fc-0a35-4183-b464-8715f1c76852",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2700/2700 [00:00<00:00, 8583.04it/s]\n"
     ]
    }
   ],
   "source": [
    "# 오류 문항들 처리\n",
    "\n",
    "train_data_path = './../input/data/train'\n",
    "train_image_path = f'{train_data_path}/images'\n",
    "\n",
    "df = pd.read_csv(f\"{train_data_path}/train.csv\")\n",
    "\n",
    "id_overlap_error = [\"003397\"]\n",
    "gender_labeling_error = ['006359', '006360', '006361', '006362', '006363', '006364']\n",
    "mask_labeling_error = ['000020', '004418', '005227']\n",
    "\n",
    "id_max = int(max(df['id']))\n",
    "id_new = id_max+1\n",
    "\n",
    "new_data_list=[]\n",
    "\n",
    "for idx in tqdm(range(len(df))):  # tqdm 을 이용하면 현재 데이터가 얼마나 처리되고 있는지 파악되어 좋습니다.\n",
    "    _path = df['path'].iloc[idx]  # 순서대로 가져와야 하기 때문에 iloc을 사용해 가져옵니다.\n",
    "    _gender = df['gender'].iloc[idx]\n",
    "    _age = df['age'].iloc[idx]\n",
    "    _id = df['id'].iloc[idx]\n",
    "\n",
    "    if _id in id_overlap_error:\n",
    "        _id='%06d'%(id_new)\n",
    "        id_new += 1\n",
    "    \n",
    "    if _id in gender_labeling_error:\n",
    "        if _gender == \"male\":\n",
    "            _gender = 'female'\n",
    "        else:\n",
    "            _gender = 'male'\n",
    "    \n",
    "    for img_name in Path(f\"{train_image_path}/{_path}\").iterdir():  # 각 dir의 이미지들을 iterative 하게 가져옵니다.\n",
    "        img_stem = img_name.stem  # 해당 파일의 파일명만을 가져옵니다. 확장자 제외.\n",
    "        if not img_stem.startswith('._'):  # avoid hidden files\n",
    "            if _id in mask_labeling_error:\n",
    "                if img_stem == \"incorrect_mask\":\n",
    "                    img_stem = 'normal'\n",
    "                elif img_stem == 'normal':\n",
    "                    img_stem = 'incorrect_mask'\n",
    "            new_data_list.append([_id, _age, _gender, img_stem, img_name.__str__()]) \n",
    "    \n",
    "df = pd.DataFrame(new_data_list)\n",
    "df.columns = ['id', 'age', 'gender', 'stem', 'img_path']\n",
    "  \n",
    "df['label'] = 0  # SET SCORE\n",
    "# AGE\n",
    "df['label'] += ((df['age'] >= 30) & (df['age'] < 60))*1\n",
    "df['label'] += (df['age'] >= 60)*2\n",
    "\n",
    "# GENDER\n",
    "df['label'] += (df['gender'] == 'female')*3\n",
    "\n",
    "# MASK wearing condition\n",
    "df['label'] += (df['stem'].isin(['incorrect_mask']))*6\n",
    "df['label'] += (df['stem'].isin(['normal']))*12\n",
    "\n",
    "df.to_csv('./labeled_data.csv', sep=',' ,na_rep='NaN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7d3edf3d-51e1-4546-8b41-1f32490b38c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전처리\n",
    "# 사진 얼굴만 crop , resize , to tensor, normalize\n",
    "def get_transforms(need=('train', 'val'), img_size=(512, 384), mean=(0.548, 0.504, 0.479), std=(0.237, 0.247, 0.246)):\n",
    "    \"\"\"\n",
    "    train 혹은 validation의 augmentation 함수를 정의합니다. train은 데이터에 많은 변형을 주어야하지만, validation에는 최소한의 전처리만 주어져야합니다.\n",
    "    \n",
    "    Args:\n",
    "        need: 'train', 혹은 'val' 혹은 둘 다에 대한 augmentation 함수를 얻을 건지에 대한 옵션입니다.\n",
    "        img_size: Augmentation 이후 얻을 이미지 사이즈입니다.\n",
    "        mean: 이미지를 Normalize할 때 사용될 RGB 평균값입니다.\n",
    "        std: 이미지를 Normalize할 때 사용될 RGB 표준편차입니다.\n",
    "\n",
    "    Returns:\n",
    "        transformations: Augmentation 함수들이 저장된 dictionary 입니다. transformations['train']은 train 데이터에 대한 augmentation 함수가 있습니다.\n",
    "    \"\"\"\n",
    "    transformations = {}\n",
    "    if 'train' in need:\n",
    "        transformations['train'] = Compose([\n",
    "            Resize(img_size[0], img_size[1], p=1.0),\n",
    "            HorizontalFlip(p=0.5),\n",
    "            RandomBrightnessContrast(brightness_limit=(-0.1, 0.1), contrast_limit=(-0.1, 0.1), p=0.5),\n",
    "            GaussNoise(p=0.5),\n",
    "            Normalize(mean=mean, std=std, max_pixel_value=255.0, p=1.0),\n",
    "            ToTensorV2(p=1.0),\n",
    "        ], p=1.0)\n",
    "    if 'val' in need:\n",
    "        transformations['val'] = Compose([\n",
    "            Resize(img_size[0], img_size[1]),\n",
    "            Normalize(mean=mean, std=std, max_pixel_value=255.0, p=1.0),\n",
    "            ToTensorV2(p=1.0),\n",
    "        ], p=1.0)\n",
    "    return transformations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6222ca3a-99ec-4e47-b87f-8e189c5815c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskBaseDataset(data.Dataset):\n",
    "    num_classes = 3 * 2 * 3\n",
    "\n",
    "    def __init__(self,image_path,label,transform=None):\n",
    "        self.image_path = image_path\n",
    "        self.label = label\n",
    "        self.transform = transform\n",
    "        \n",
    "    def set_transform(self, transform):\n",
    "        \"\"\"\n",
    "        transform 함수를 설정하는 함수입니다.\n",
    "        \"\"\"\n",
    "        self.transform = transform\n",
    "        \n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        데이터를 불러오는 함수입니다. \n",
    "        데이터셋 class에 데이터 정보가 저장되어 있고, index를 통해 해당 위치에 있는 데이터 정보를 불러옵니다.\n",
    "        \n",
    "        Args:\n",
    "            index: 불러올 데이터의 인덱스값입니다.\n",
    "        \"\"\"\n",
    "        # 이미지를 불러옵니다.\n",
    "        image = Image.open(image_path[index])\n",
    "        \n",
    "        # 이미지를 Augmentation 시킵니다.\n",
    "        image_transform = self.transform(image=np.array(image))['image']\n",
    "        return image_transform, self.label[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3d38c752-631b-464c-b17b-379470fc1fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정의한 Augmentation 함수와 Dataset 클래스 객체를 생성합니다.\n",
    "transform = get_transforms\n",
    "image_path = pd.read_csv(\"labeled_data.csv\").img_path\n",
    "label = pd.read_csv(\"labeled_data.csv\").label\n",
    "\n",
    "dataset = MaskBaseDataset(image_path,label)\n",
    "\n",
    "# train dataset과 validation dataset을 8:2 비율로 나눕니다.\n",
    "n_val = int(len(dataset) * 0.2)\n",
    "n_train = len(dataset) - n_val\n",
    "train_dataset, val_dataset = data.random_split(dataset, [n_train, n_val])\n",
    "\n",
    "\n",
    "# 각 dataset에 augmentation 함수를 설정합니다.\n",
    "train_dataset.dataset.set_transform(transform(need = 'train'))\n",
    "val_dataset.dataset.set_transform(transform(need = \"val\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
