{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "about-heavy",
   "metadata": {},
   "source": [
    "## 0. Libarary 불러오기 및 경로설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "cubic-scoop",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import Resize, ToTensor, Normalize, RandomHorizontalFlip, CenterCrop\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "a9620ecb-d78f-42eb-a9ea-291f2dedeff9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/ml/code\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "5bcf4900-37b1-40aa-9dbf-ac88573377a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EDA.ipynb  mask.ipynb  models  requirements.txt  sample_submission.ipynb\n"
     ]
    }
   ],
   "source": [
    "!cd /opt/ml/input/data/train/images\n",
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c40701a1-db28-4cd9-8a11-c71be82e740a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "41b7b149-f33e-435e-b06a-79f9ddb43346",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "img_dir = '/opt/ml/input/data/train/images'\n",
    "info_dir = '/opt/ml/input/data/train/train.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "b9c4063f-71ef-481b-9df6-16421e9bdada",
   "metadata": {},
   "outputs": [],
   "source": [
    "info = pd.read_csv(info_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "5b12d5c7-e817-4de2-ba13-63001ca49b5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>gender</th>\n",
       "      <th>race</th>\n",
       "      <th>age</th>\n",
       "      <th>path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000001</td>\n",
       "      <td>female</td>\n",
       "      <td>Asian</td>\n",
       "      <td>45</td>\n",
       "      <td>000001_female_Asian_45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000002</td>\n",
       "      <td>female</td>\n",
       "      <td>Asian</td>\n",
       "      <td>52</td>\n",
       "      <td>000002_female_Asian_52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000004</td>\n",
       "      <td>male</td>\n",
       "      <td>Asian</td>\n",
       "      <td>54</td>\n",
       "      <td>000004_male_Asian_54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>000005</td>\n",
       "      <td>female</td>\n",
       "      <td>Asian</td>\n",
       "      <td>58</td>\n",
       "      <td>000005_female_Asian_58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>000006</td>\n",
       "      <td>female</td>\n",
       "      <td>Asian</td>\n",
       "      <td>59</td>\n",
       "      <td>000006_female_Asian_59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2695</th>\n",
       "      <td>006954</td>\n",
       "      <td>male</td>\n",
       "      <td>Asian</td>\n",
       "      <td>19</td>\n",
       "      <td>006954_male_Asian_19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2696</th>\n",
       "      <td>006955</td>\n",
       "      <td>male</td>\n",
       "      <td>Asian</td>\n",
       "      <td>19</td>\n",
       "      <td>006955_male_Asian_19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2697</th>\n",
       "      <td>006956</td>\n",
       "      <td>male</td>\n",
       "      <td>Asian</td>\n",
       "      <td>19</td>\n",
       "      <td>006956_male_Asian_19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2698</th>\n",
       "      <td>006957</td>\n",
       "      <td>male</td>\n",
       "      <td>Asian</td>\n",
       "      <td>20</td>\n",
       "      <td>006957_male_Asian_20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2699</th>\n",
       "      <td>006959</td>\n",
       "      <td>male</td>\n",
       "      <td>Asian</td>\n",
       "      <td>19</td>\n",
       "      <td>006959_male_Asian_19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2700 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id  gender   race  age                    path\n",
       "0     000001  female  Asian   45  000001_female_Asian_45\n",
       "1     000002  female  Asian   52  000002_female_Asian_52\n",
       "2     000004    male  Asian   54    000004_male_Asian_54\n",
       "3     000005  female  Asian   58  000005_female_Asian_58\n",
       "4     000006  female  Asian   59  000006_female_Asian_59\n",
       "...      ...     ...    ...  ...                     ...\n",
       "2695  006954    male  Asian   19    006954_male_Asian_19\n",
       "2696  006955    male  Asian   19    006955_male_Asian_19\n",
       "2697  006956    male  Asian   19    006956_male_Asian_19\n",
       "2698  006957    male  Asian   20    006957_male_Asian_20\n",
       "2699  006959    male  Asian   19    006959_male_Asian_19\n",
       "\n",
       "[2700 rows x 5 columns]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "661cc214-a1d6-4ced-ae5b-73bfa01085df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b7c27c1-6a2b-4141-903e-1dd4c2561664",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "de729fcc-351c-482f-aff6-26af2dba9f70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       000001_female_Asian_45\n",
       "1       000002_female_Asian_52\n",
       "2         000004_male_Asian_54\n",
       "3       000005_female_Asian_58\n",
       "4       000006_female_Asian_59\n",
       "                 ...          \n",
       "2695      006954_male_Asian_19\n",
       "2696      006955_male_Asian_19\n",
       "2697      006956_male_Asian_19\n",
       "2698      006957_male_Asian_20\n",
       "2699      006959_male_Asian_19\n",
       "Name: path, Length: 2700, dtype: object"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "person_dir = info['path']\n",
    "person_dir\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "555ad3b8-ec78-4d2f-b897-9e5a7549897e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "6ebaafad-1d5d-43fd-b5a3-d4017cd29f6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "female 45\n"
     ]
    }
   ],
   "source": [
    "# split test to get gender and age\n",
    "pdir = person_dir[0]\n",
    "split_str = pdir.split('_')\n",
    "gender = split_str[1]\n",
    "age = split_str[3]\n",
    "\n",
    "print(gender, age)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd9ea5b8-0966-405c-8797-e2d1b38eda44",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "daa15714-a942-4e42-b9d8-3996ff7648b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# numerize classes function\n",
    "\n",
    "def get_class(dict):\n",
    "    if dict['mask'] == 0:#wear\n",
    "        if dict['gen'] == 'male':\n",
    "            if dict['age'] < 30:\n",
    "                return 0\n",
    "            elif 30 <= dict['age'] < 60:\n",
    "                return 1\n",
    "            else:\n",
    "                return 2\n",
    "        else:\n",
    "            if dict['age'] < 30:\n",
    "                return 3\n",
    "            elif 30 <= dict['age'] < 60:\n",
    "                return 4\n",
    "            else:\n",
    "                return 5\n",
    "    elif dict['mask'] == 1:#incorrect\n",
    "        if dict['gen'] == 'male':\n",
    "            if dict['age'] < 30:\n",
    "                return 6\n",
    "            elif 30 <= dict['age'] < 60:\n",
    "                return 7\n",
    "            else:\n",
    "                return 8\n",
    "        else:\n",
    "            if dict['age'] < 30:\n",
    "                return 9\n",
    "            elif 30 <= dict['age'] < 60:\n",
    "                return 10\n",
    "            else:\n",
    "                return 11\n",
    "    elif dict['mask'] == 2:#normal\n",
    "        if dict['gen'] == 'male':\n",
    "            if dict['age'] < 30:\n",
    "                return 12\n",
    "            elif 30 <= dict['age'] < 60:\n",
    "                return 13\n",
    "            else:\n",
    "                return 14\n",
    "        else:\n",
    "            if dict['age'] < 30:\n",
    "                return 15\n",
    "            elif 30 <= dict['age'] < 60:\n",
    "                return 16\n",
    "            else:\n",
    "                return 17\n",
    "\n",
    "get_class({'mask':0, 'gen':'male', 'age':64})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1dcbf15-1e4e-4256-b3c7-44f4a4319f8c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "ccbcfebd-1aa4-465a-bf42-33920703d684",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['incorrect_mask', 'normal', 'mask1', 'mask2', 'mask3', 'mask4', 'mask5']\n",
      "18900\n",
      "/opt/ml/input/data/train/images/000001_female_Asian_45/incorrect_mask  ->  10\n",
      "/opt/ml/input/data/train/images/000001_female_Asian_45/normal  ->  16\n",
      "/opt/ml/input/data/train/images/000001_female_Asian_45/mask1  ->  4\n",
      "/opt/ml/input/data/train/images/000001_female_Asian_45/mask2  ->  4\n",
      "/opt/ml/input/data/train/images/000001_female_Asian_45/mask3  ->  4\n",
      "/opt/ml/input/data/train/images/000001_female_Asian_45/mask4  ->  4\n",
      "/opt/ml/input/data/train/images/000001_female_Asian_45/mask5  ->  4\n",
      "/opt/ml/input/data/train/images/000002_female_Asian_52/incorrect_mask  ->  10\n",
      "/opt/ml/input/data/train/images/000002_female_Asian_52/normal  ->  16\n",
      "/opt/ml/input/data/train/images/000002_female_Asian_52/mask1  ->  4\n",
      "/opt/ml/input/data/train/images/000002_female_Asian_52/mask2  ->  4\n",
      "/opt/ml/input/data/train/images/000002_female_Asian_52/mask3  ->  4\n",
      "/opt/ml/input/data/train/images/000002_female_Asian_52/mask4  ->  4\n",
      "/opt/ml/input/data/train/images/000002_female_Asian_52/mask5  ->  4\n",
      "/opt/ml/input/data/train/images/000004_male_Asian_54/incorrect_mask  ->  7\n",
      "/opt/ml/input/data/train/images/000004_male_Asian_54/normal  ->  13\n",
      "/opt/ml/input/data/train/images/000004_male_Asian_54/mask1  ->  1\n",
      "/opt/ml/input/data/train/images/000004_male_Asian_54/mask2  ->  1\n",
      "/opt/ml/input/data/train/images/000004_male_Asian_54/mask3  ->  1\n",
      "/opt/ml/input/data/train/images/000004_male_Asian_54/mask4  ->  1\n",
      "/opt/ml/input/data/train/images/000004_male_Asian_54/mask5  ->  1\n",
      "/opt/ml/input/data/train/images/000005_female_Asian_58/incorrect_mask  ->  10\n",
      "/opt/ml/input/data/train/images/000005_female_Asian_58/normal  ->  16\n",
      "/opt/ml/input/data/train/images/000005_female_Asian_58/mask1  ->  4\n",
      "/opt/ml/input/data/train/images/000005_female_Asian_58/mask2  ->  4\n",
      "/opt/ml/input/data/train/images/000005_female_Asian_58/mask3  ->  4\n",
      "/opt/ml/input/data/train/images/000005_female_Asian_58/mask4  ->  4\n",
      "/opt/ml/input/data/train/images/000005_female_Asian_58/mask5  ->  4\n",
      "/opt/ml/input/data/train/images/000006_female_Asian_59/incorrect_mask  ->  10\n",
      "/opt/ml/input/data/train/images/000006_female_Asian_59/normal  ->  16\n"
     ]
    }
   ],
   "source": [
    "# create image_paths\\\n",
    "# current status: eacp person, each mask or not.\n",
    "# need to sepereate them to a state that (one eg, one label).\n",
    "\n",
    "# may the label process be here? or in dataset? here: two functions in one cell. dataset: time taking and messy index to split.\n",
    "img_paths = []\n",
    "status = ['incorrect_mask', 'normal']\n",
    "\n",
    "labels = []\n",
    "\n",
    "for i in range(5):\n",
    "    status.append('mask' + str(i+1))\n",
    "print(status)\n",
    "\n",
    "for p_dir in person_dir:\n",
    "    \n",
    "    split_str = p_dir.split('_')\n",
    "    gender = split_str[1]\n",
    "    age = split_str[3]\n",
    "    \n",
    "    for idx, st in enumerate(status):\n",
    "        img_paths.append(img_dir + \"/\" + p_dir + \"/\" + st)\n",
    "\n",
    "        label_stat = {}\n",
    "        if idx == 0:\n",
    "            label_stat['mask'] = 1\n",
    "        elif idx == 1:\n",
    "            label_stat['mask'] = 2\n",
    "        else:\n",
    "            label_stat['mask'] = 0\n",
    "        \n",
    "        label_stat['age'] = int(age)\n",
    "        label_stat['gen'] = gender\n",
    "        \n",
    "        labels.append(get_class(label_stat))\n",
    "        \n",
    "        \n",
    "        \n",
    "print(len(img_paths))\n",
    "for i in range(10*3):\n",
    "    print(img_paths[i], \" -> \", labels[i])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21bc4949-cf51-4e90-9abd-b14f6e703423",
   "metadata": {},
   "source": [
    "# create only normal and incorrect dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "b0a1306f-16bb-49b8-8a26-2eafe3eafdbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5400\n",
      "/opt/ml/input/data/train/images/000001_female_Asian_45/incorrect_mask  ->  10\n",
      "/opt/ml/input/data/train/images/000001_female_Asian_45/normal  ->  16\n",
      "/opt/ml/input/data/train/images/000002_female_Asian_52/incorrect_mask  ->  10\n",
      "/opt/ml/input/data/train/images/000002_female_Asian_52/normal  ->  16\n",
      "/opt/ml/input/data/train/images/000004_male_Asian_54/incorrect_mask  ->  7\n"
     ]
    }
   ],
   "source": [
    "img_paths_false = []\n",
    "status = ['incorrect_mask', 'normal']\n",
    "\n",
    "labels_false = []\n",
    "\n",
    "# for i in range(5):\n",
    "#     status.append('mask' + str(i+1))\n",
    "# print(status)\n",
    "\n",
    "for p_dir in person_dir:\n",
    "    \n",
    "    split_str = p_dir.split('_')\n",
    "    gender = split_str[1]\n",
    "    age = split_str[3]\n",
    "    \n",
    "    for idx, st in enumerate(status):\n",
    "        img_paths_false.append(img_dir + \"/\" + p_dir + \"/\" + st)\n",
    "\n",
    "        label_stat = {}\n",
    "        if idx == 0:\n",
    "            label_stat['mask'] = 1\n",
    "        elif idx == 1:\n",
    "            label_stat['mask'] = 2\n",
    "        else:\n",
    "            label_stat['mask'] = 0\n",
    "        \n",
    "        label_stat['age'] = int(age)\n",
    "        label_stat['gen'] = gender\n",
    "        \n",
    "        labels_false.append(get_class(label_stat))\n",
    "        \n",
    "        \n",
    "        \n",
    "print(len(img_paths_false))\n",
    "for i in range(5):\n",
    "    print(img_paths_false[i], \" -> \", labels_false[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e7cd20-052b-4c37-8596-cedf02fc33db",
   "metadata": {},
   "source": [
    "# Only positive data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "6fb17601-8f0c-4435-a60d-d73a6a8188c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['mask1', 'mask2', 'mask3', 'mask4', 'mask5']\n",
      "13500\n",
      "/opt/ml/input/data/train/images/000001_female_Asian_45/mask1  ->  10\n",
      "/opt/ml/input/data/train/images/000001_female_Asian_45/mask2  ->  16\n",
      "/opt/ml/input/data/train/images/000001_female_Asian_45/mask3  ->  4\n",
      "/opt/ml/input/data/train/images/000001_female_Asian_45/mask4  ->  4\n",
      "/opt/ml/input/data/train/images/000001_female_Asian_45/mask5  ->  4\n"
     ]
    }
   ],
   "source": [
    "img_paths_true = []\n",
    "status = []\n",
    "\n",
    "labels_true = []\n",
    "\n",
    "for i in range(5):\n",
    "    status.append('mask' + str(i+1))\n",
    "print(status)\n",
    "\n",
    "for p_dir in person_dir:\n",
    "    \n",
    "    split_str = p_dir.split('_')\n",
    "    gender = split_str[1]\n",
    "    age = split_str[3]\n",
    "    \n",
    "    for idx, st in enumerate(status):\n",
    "        img_paths_true.append(img_dir + \"/\" + p_dir + \"/\" + st)\n",
    "\n",
    "        label_stat = {}\n",
    "        if idx == 0:\n",
    "            label_stat['mask'] = 1\n",
    "        elif idx == 1:\n",
    "            label_stat['mask'] = 2\n",
    "        else:\n",
    "            label_stat['mask'] = 0\n",
    "        \n",
    "        label_stat['age'] = int(age)\n",
    "        label_stat['gen'] = gender\n",
    "        \n",
    "        labels_true.append(get_class(label_stat))\n",
    "        \n",
    "        \n",
    "        \n",
    "print(len(img_paths_true))\n",
    "for i in range(5):\n",
    "    print(img_paths_true[i], \" -> \", labels_true[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "d93a0957-102f-4399-a5a9-a488052123d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskDataset(Dataset):\n",
    "    def __init__(self, img_paths, labels, transform = None):\n",
    "        self.img_paths = img_paths\n",
    "        self.transform = transform\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        pth = self.img_paths[index]\n",
    "        \n",
    "        if os.path.exists(pth + '.jpg'):\n",
    "            self.img_paths[index] = pth + '.jpg'\n",
    "        elif os.path.exists(pth + '.png'):\n",
    "            self.img_paths[index] = pth + '.png'\n",
    "        elif os.path.exists(pth + '.jpeg'):\n",
    "            self.img_paths[index] = pth + '.jpeg'\n",
    "            \n",
    "        image = Image.open(self.img_paths[index])\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        else:\n",
    "#             image = transforms.Resize((64,64))(image)\n",
    "            image = transforms.ToTensor()(image)\n",
    "        return image, self.labels[index]\n",
    "\n",
    "    def show_pic(self, index):\n",
    "        image = Image.open(self.img_paths[index])\n",
    "#         image = transforms.Resize((64,64))(image)\n",
    "        return image\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_paths)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return \"number of dataset : \" + str(self.__len__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "2e95bfda-bb43-4603-848a-67654b6bb875",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transforms_img(im):\n",
    "    ######################################TODO######################################\n",
    "    im = transforms.ToTensor()(im)\n",
    "    im = transforms.Resize((224,224))(im)\n",
    "    im = transforms.RandomHorizontalFlip(0.5)(im)\n",
    "    im = transforms.CenterCrop((150,150))(im)\n",
    "\n",
    "    ################################################################################\n",
    "    return im"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26eaee52-bc41-45e1-8aa2-d1a3876d20b4",
   "metadata": {},
   "source": [
    "# Test speed of data generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "7426e97e-e420-4970-988e-6bd4f438f6e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # test speed of data generator without transforms\n",
    "# t = MaskDataset(img_paths, labels)\n",
    "# for idx, d in enumerate(tqdm(t)):\n",
    "#     pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "c4d5ef16-34c9-4e8e-9961-41a3a914e987",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # test speed of data generator with transforms. totensor, resize, randomhorizontalflip, centercrop\n",
    "# t = MaskDataset(img_paths, labels, get_transforms_img)\n",
    "# for idx, d in enumerate(tqdm(t)):\n",
    "#     pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "dbbe3a95-7458-4732-8beb-b4c41e9b84a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # test speed of data generator with transforms. totensor, resize, randomhorizontalflip, centercrop\n",
    "# t = MaskDataset(img_paths, labels)\n",
    "# for idx, d in enumerate(tqdm(t)):\n",
    "#     pass\n",
    "\n",
    "# composed = transforms.Compose([\n",
    "#     ToTensor(),\n",
    "#     Resize((224,224)),\n",
    "#     RandomHorizontalFlip(0.5),\n",
    "#     CenterCrop((150,150))\n",
    "# ])\n",
    "\n",
    "# # test speed of data generator with transforms. totensor, resize, randomhorizontalflip, centercrop\n",
    "# t = MaskDataset(img_paths, labels, composed)\n",
    "# for idx, d in enumerate(tqdm(t)):\n",
    "#     pass\n",
    "\n",
    "# composed = transforms.Compose([\n",
    "\n",
    "#     Resize((224,224)),\n",
    "#     RandomHorizontalFlip(0.5),\n",
    "#     CenterCrop((150,150)),\n",
    "#     ToTensor()\n",
    "# ])\n",
    "\n",
    "# # test speed of data generator with transforms. totensor, resize, randomhorizontalflip, centercrop\n",
    "# t = MaskDataset(img_paths, labels, composed)\n",
    "# for idx, d in enumerate(tqdm(t)):\n",
    "#     pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b6697cb-196a-4754-9753-0a2bc7cd09c2",
   "metadata": {},
   "source": [
    "# Concatinate dataset with correct, incorrect, normal.\n",
    "lets deal with the imbalance!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "cb7d2841-0a44-4099-b554-0d1521c90174",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "number of dataset : 18900"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = MaskDataset(img_paths, labels, get_transforms_img)\n",
    "# dataset = MaskDataset(img_paths, labels) # 0.87 0.81 f1\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "f4741c86-4846-4ab2-8911-0a9b5cde8ad8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5300\n",
      "100\n",
      "13400\n",
      "100\n",
      "18700\n",
      "200\n"
     ]
    }
   ],
   "source": [
    "# true and false dataset\n",
    "\n",
    "# false_dataset = MaskDataset(img_paths_false, labels_false, get_transforms_img)\n",
    "# true_dataset = MaskDataset(img_paths_true, labels_true, get_transforms_img)\n",
    "\n",
    "false_dataset = MaskDataset(img_paths_false, labels_false)\n",
    "true_dataset = MaskDataset(img_paths_true, labels_true)\n",
    "\n",
    "\n",
    "false_len = false_dataset.__len__()\n",
    "true_len = true_dataset.__len__()\n",
    "# int(false_len  * (1/3))\n",
    "\n",
    "\n",
    "false_dataset, false_testset = torch.utils.data.random_split(false_dataset, [ int(false_len) - 100, 100 ], generator=torch.Generator().manual_seed(42))\n",
    "true_dataset, true_testset = torch.utils.data.random_split(true_dataset, [ int(true_len) - 100, 100 ], generator = torch.Generator().manual_seed(42))\n",
    "\n",
    "print(false_dataset.__len__())\n",
    "print(false_testset.__len__())\n",
    "print(true_dataset.__len__())\n",
    "print(true_testset.__len__())\n",
    "\n",
    "concat_dataset = torch.utils.data.ConcatDataset([false_dataset, true_dataset])\n",
    "concat_testset = torch.utils.data.ConcatDataset([false_testset, true_testset])\n",
    "\n",
    "print(len(concat_dataset))\n",
    "print(len(concat_testset))\n",
    "\n",
    "concat_dataloader = torch.utils.data.DataLoader(concat_dataset, batch_size = 64, shuffle = True, num_workers = 2)\n",
    "concat_testloader = torch.utils.data.DataLoader(concat_testset, batch_size = 64, shuffle = True, num_workers = 2)\n",
    "\n",
    "\n",
    "# print(\"false data set len = \", false_dataset.__len__())\n",
    "# print(\"true data set len = \", true_dataset.__len__())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "72499deb-010d-45f4-9f27-d240c0aff21a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset, testset = torch.utils.data.random_split(dataset, [18000,900], generator=torch.Generator().manual_seed(42))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "740bc3f1-ddc7-46ee-9bb6-a1fb4acf93ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18000"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "66eeb8a5-185c-4a3f-af84-c66f41a1f902",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset.show_pic(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3643571b-510b-41c0-be8f-ac4e50ecca55",
   "metadata": {},
   "source": [
    "# issue: there are png, jpeg, pjg multiple extensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "af45d56c-5f56-4ef8-9b6a-03034b5a0986",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>gender</th>\n",
       "      <th>race</th>\n",
       "      <th>age</th>\n",
       "      <th>path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2311</th>\n",
       "      <td>006148</td>\n",
       "      <td>male</td>\n",
       "      <td>Asian</td>\n",
       "      <td>19</td>\n",
       "      <td>006148_male_Asian_19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id gender   race  age                  path\n",
       "2311  006148   male  Asian   19  006148_male_Asian_19"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#png file test\n",
    "info[info['id'] == '006148']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "caa15798-2dc1-46e4-8328-51a6f45ae01d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>gender</th>\n",
       "      <th>race</th>\n",
       "      <th>age</th>\n",
       "      <th>path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2560</th>\n",
       "      <td>006582</td>\n",
       "      <td>female</td>\n",
       "      <td>Asian</td>\n",
       "      <td>20</td>\n",
       "      <td>006582_female_Asian_20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id  gender   race  age                    path\n",
       "2560  006582  female  Asian   20  006582_female_Asian_20"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info[info['id'] == '006582']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "34e07fe8-e20d-495f-b19a-79591cc2b065",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 150, 150])"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.__getitem__(2560*7)[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "fd6efa1b-f9e0-4ac6-89d0-97453db88906",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 150, 150])"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.__getitem__(1)[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "ccc1cc7c-db32-42c2-be00-4f6f80b9aaaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[0.8438, 0.8440, 0.8440,  ..., 0.8353, 0.8353, 0.8353],\n",
       "          [0.8478, 0.8490, 0.8490,  ..., 0.8373, 0.8373, 0.8373],\n",
       "          [0.8462, 0.8471, 0.8471,  ..., 0.8392, 0.8392, 0.8392],\n",
       "          ...,\n",
       "          [0.1166, 0.1137, 0.1098,  ..., 0.1473, 0.1482, 0.1467],\n",
       "          [0.1261, 0.1157, 0.1083,  ..., 0.1423, 0.1431, 0.1408],\n",
       "          [0.1120, 0.1052, 0.1082,  ..., 0.1449, 0.1385, 0.1363]],\n",
       " \n",
       "         [[0.8124, 0.8126, 0.8126,  ..., 0.8039, 0.8039, 0.8039],\n",
       "          [0.8164, 0.8176, 0.8176,  ..., 0.8059, 0.8059, 0.8059],\n",
       "          [0.8148, 0.8157, 0.8157,  ..., 0.8078, 0.8078, 0.8078],\n",
       "          ...,\n",
       "          [0.1166, 0.1137, 0.1098,  ..., 0.1473, 0.1482, 0.1467],\n",
       "          [0.1261, 0.1157, 0.1083,  ..., 0.1423, 0.1431, 0.1408],\n",
       "          [0.1120, 0.1052, 0.1082,  ..., 0.1449, 0.1385, 0.1363]],\n",
       " \n",
       "         [[0.7693, 0.7695, 0.7695,  ..., 0.7608, 0.7608, 0.7608],\n",
       "          [0.7732, 0.7745, 0.7745,  ..., 0.7627, 0.7627, 0.7627],\n",
       "          [0.7717, 0.7725, 0.7725,  ..., 0.7647, 0.7647, 0.7647],\n",
       "          ...,\n",
       "          [0.1558, 0.1529, 0.1490,  ..., 0.1866, 0.1874, 0.1859],\n",
       "          [0.1653, 0.1549, 0.1475,  ..., 0.1815, 0.1824, 0.1800],\n",
       "          [0.1512, 0.1444, 0.1474,  ..., 0.1841, 0.1777, 0.1756]]]),\n",
       " 3)"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.__getitem__(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "8b28acf2-3a6b-46cb-bddd-9c10a8b70bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size = 64, shuffle = True, num_workers = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "ea13edba-0d33-473d-aa3f-a850dd6a8e01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 3, 150, 150])"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iter(dataloader).next()[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adf29ded-991b-432c-b51f-a63105b4d73b",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "794b3408-f108-4db3-ac77-804d93b76799",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install ipywidgets\n",
    "#!jupyter nbextension enable --py widgetsnbextension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "4d16b094-0f46-481b-87e6-5e49880fc437",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# resnet test\n",
    "import torchvision\n",
    "mask_resnet_18 = torchvision.models.resnet18(pretrained = True)\n",
    "# mask_resnet_18.fc = torch.nn.Linear(512, 18, bias = True)\n",
    "# torch.nn.init.xavier_uniform_(mask_resnet_18.fc.weight)\n",
    "# mask_resnet_18.fc.bias.data.fill_(0.01)\n",
    "mask_resnet_18\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "3d948f5c-7a5f-4fe4-8022-8a38a061c9ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskResnot(nn.Module):\n",
    "    def __init__(self, num_classes: int = 18):\n",
    "        super(MaskResnot, self).__init__()\n",
    "        self.resnet_18 = mask_resnet_18\n",
    "        \n",
    "        \n",
    "#         self.features = nn.Sequential(\n",
    "#             nn.Conv2d(3, 64, kernel_size=11, stride=4, padding=2),\n",
    "#             nn.BatchNorm2d(64),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#         )\n",
    "#         self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "#         self.classifier = nn.Sequential(\n",
    "#             nn.Dropout(),\n",
    "#             nn.Linear(64, 32),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.Linear(32, num_classes),\n",
    "#         )\n",
    "    def init_params(self):\n",
    "        torch.nn.init.xavier_uniform_(self.resnet_18.fc.weight)\n",
    "        self.resnet_18.fc.bias.data.fill_(0.01)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.resnet_18(x)\n",
    "#         x = self.features(x)\n",
    "#         x = self.avgpool(x)\n",
    "#         x = torch.flatten(x, 1)\n",
    "#         x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "6ecbd925-af53-4e53-b87c-58482c0bc343",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85d25bc0-a7de-4bee-b8fd-f98b3e0f762e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "80d5e113-367a-4a38-a760-01d9fee08334",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MaskResnot(18).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "8c1e9f98-4d26-4cbf-ac90-cc6964158ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "7319a882-76a8-4dae-b034-cf69af04f7bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = nn.CrossEntropyLoss()\n",
    "opt = optim.Adam(model.parameters(), lr = 1e-3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "8a26e448-3900-4b7f-b82a-4fd296178c59",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/293 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch number : 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 293/293 [00:56<00:00,  5.16it/s]\n",
      "  0%|          | 0/293 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18700\n",
      "18700\n",
      "loss :  0.032970285300902506\n",
      "acc:  0.34411764705882353\n",
      "epoch number : 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 293/293 [00:57<00:00,  5.13it/s]\n",
      "  0%|          | 0/293 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18700\n",
      "18700\n",
      "loss :  0.02337036753720778\n",
      "acc:  0.5368449197860963\n",
      "epoch number : 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 293/293 [00:57<00:00,  5.14it/s]\n",
      "  0%|          | 0/293 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18700\n",
      "18700\n",
      "loss :  0.020194163975868634\n",
      "acc:  0.5977005347593582\n",
      "epoch number : 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 293/293 [00:57<00:00,  5.13it/s]\n",
      "  0%|          | 0/293 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18700\n",
      "18700\n",
      "loss :  0.018384154671653708\n",
      "acc:  0.6285026737967915\n",
      "epoch number : 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 293/293 [00:56<00:00,  5.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18700\n",
      "18700\n",
      "loss :  0.016966310239093188\n",
      "acc:  0.661283422459893\n",
      "epoch = 5 done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "epochs = 5\n",
    "model.init_params()\n",
    "loss_record = []\n",
    "\n",
    "for p in model.parameters():\n",
    "    p.requires_grad = False\n",
    "for p in model.resnet_18.fc.parameters():\n",
    "# for p in model.fc.parameters():\n",
    "    p.requires_grad = True\n",
    "\n",
    "\n",
    "\n",
    "for e in range(epochs):\n",
    "    print(\"epoch number :\", e)\n",
    "    running_loss = 0\n",
    "    running_acc = 0\n",
    "    n_size = 0\n",
    "    for idx, (batch_in, batch_out) in enumerate(tqdm(concat_dataloader)):\n",
    "        X = batch_in.to(device)\n",
    "        Y = batch_out.to(device)\n",
    "        \n",
    "        n_size += X.size(0)\n",
    "        logits = model(X)\n",
    "        loss_out = loss(logits, Y)\n",
    "        _, preds = torch.max(logits, 1)\n",
    "        \n",
    "        opt.zero_grad()\n",
    "        loss_out.backward()\n",
    "        opt.step()\n",
    "        running_loss += loss_out.item() #* batch_in.size(0)\n",
    "        \n",
    "        running_acc += torch.sum(preds == Y.data).item()\n",
    "        \n",
    "    loss_val_avg = running_loss / len(concat_dataset)\n",
    "    loss_record.append(loss_val_avg)\n",
    "    print(len(concat_dataset))\n",
    "    print(n_size)\n",
    "    print(\"loss : \", loss_val_avg)\n",
    "    print(\"acc: \", running_acc/ n_size)\n",
    "# torch.save(model, './models/'+str(epochs)+ \"_concat_\"+'resnet_18.pt')\n",
    "print(\"epoch = %d done!\" %(epochs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "e4b33951-7c6b-4ebe-a44d-75d64abcf966",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "bfa76a02-02b0-46f1-aa84-3dae69fe640d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "c62795b4-a1f3-4328-92d3-fb193b1c3655",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.032970285300902506,\n",
       " 0.02337036753720778,\n",
       " 0.020194163975868634,\n",
       " 0.018384154671653708,\n",
       " 0.016966310239093188]"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "95b1d69d-3b97-426d-a550-281f7872086a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f7fa4e89940>]"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(1,1)\n",
    "ax.plot([e+1 for e in range(epochs)], loss_record)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "908d17cb-26c0-468a-862d-12187285f8b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "c1f5c5cb-944d-4d19-9d99-f30b66e2dfd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:01<00:00, 104.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy =  0.63\n",
      "mirco average:  0.63\n",
      "marco average:  0.4772511950285387\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "num_acc = 0\n",
    "\n",
    "y_true = []\n",
    "y_pred = []\n",
    "# print(len(false_testset))\n",
    "# print(len(concat_testloader))\n",
    "# for idx, (batch_in, batch_out) in enumerate(tqdm(concat_testloader)):\n",
    "for idx, (batch_in, batch_out) in enumerate(tqdm(DataLoader(concat_testset))):\n",
    "    with torch.no_grad():\n",
    "        X = batch_in.to(device)\n",
    "        Y = batch_out.to(device)\n",
    "        \n",
    "        logit = model(X)\n",
    "        _, preds = torch.max(logit, 1)\n",
    "        \n",
    "        num_acc += torch.sum(preds == Y.data)\n",
    "        \n",
    "        y_pred += preds.cpu().detach().numpy().tolist()\n",
    "        y_true += Y.cpu().detach().numpy().tolist()\n",
    "        \n",
    "print(\"accuracy = \", num_acc.item() / len(concat_testset))\n",
    "score = f1_score(y_true, y_pred, average = 'micro')\n",
    "print(\"mirco average: \", score)\n",
    "score = f1_score(y_true, y_pred, average = 'macro')\n",
    "print(\"marco average: \", score)\n",
    "# print(y_pred)\n",
    "# print(y_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e48e978c-ecef-4504-907c-0955c26a436f",
   "metadata": {},
   "source": [
    "# leader board\n",
    "without data augmentation(only use ToTensor())\n",
    "* epoch = 1; acc: 68\n",
    "* epoch = 3; acc: 87\n",
    "* epoch = 3; micro f1: 0.87, macro f1:0.81 \n",
    "\n",
    "so far without data agumentation.\n",
    "\n",
    "with data augmentaion,\n",
    "* epoch = 3, micro f1: 86, macro f1:74\n",
    "* epoch = 5. micro f1:91, macro f1:77\n",
    "* epooch = 10. micro 95, micro 93\n",
    "* epoch = 20, micro 96, macro 94\n",
    "* epoch = 40, micro 98, macro 96\n",
    "\n",
    "\n",
    "concat dataset\n",
    "* epoch = 3, micro 83, macro 82\n",
    "* epoch = 5, micro 90, macro 91\n",
    "* epoch = 20, micro 95, macro 93\n",
    "* epoch = 30, micro 95, macro 95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "built-elevation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트 데이터셋 폴더 경로를 지정해주세요.\n",
    "test_dir = '/opt/ml/input/data/eval'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quiet-organizer",
   "metadata": {},
   "source": [
    "## 1. Model 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "acknowledged-easter",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(nn.Module):\n",
    "    def __init__(self, num_classes: int = 1000):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=11, stride=4, padding=2),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(32, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.features(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "domestic-channels",
   "metadata": {},
   "source": [
    "## 2. Test Dataset 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "extensive-north",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestDataset(Dataset):\n",
    "    def __init__(self, img_paths, transform):\n",
    "        self.img_paths = img_paths\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image = Image.open(self.img_paths[index])\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "continued-feelings",
   "metadata": {},
   "source": [
    "## 3. Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "coral-shade",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test inference is done!\n"
     ]
    }
   ],
   "source": [
    "# meta 데이터와 이미지 경로를 불러옵니다.\n",
    "submission = pd.read_csv(os.path.join(test_dir, 'info.csv'))\n",
    "image_dir = os.path.join(test_dir, 'images')\n",
    "\n",
    "# Test Dataset 클래스 객체를 생성하고 DataLoader를 만듭니다.\n",
    "image_paths = [os.path.join(image_dir, img_id) for img_id in submission.ImageID]\n",
    "transform = transforms.Compose([\n",
    "    Resize((512, 384), Image.BILINEAR),\n",
    "    ToTensor(),\n",
    "    Normalize(mean=(0.5, 0.5, 0.5), std=(0.2, 0.2, 0.2)),\n",
    "])\n",
    "dataset = TestDataset(image_paths, transform)\n",
    "\n",
    "loader = DataLoader(\n",
    "    dataset,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# 모델을 정의합니다. (학습한 모델이 있다면 torch.load로 모델을 불러주세요!)\n",
    "device = torch.device('cuda')\n",
    "# model = MyModel(num_classes=18).to(device)\n",
    "model.eval()\n",
    "\n",
    "# 모델이 테스트 데이터셋을 예측하고 결과를 저장합니다.\n",
    "all_predictions = []\n",
    "for images in loader:\n",
    "    with torch.no_grad():\n",
    "        images = images.to(device)\n",
    "        pred = model(images)\n",
    "        pred = pred.argmax(dim=-1)\n",
    "        all_predictions.extend(pred.cpu().numpy())\n",
    "submission['ans'] = all_predictions\n",
    "\n",
    "# 제출할 파일을 저장합니다.\n",
    "submission.to_csv(os.path.join(test_dir, 'submission.csv'), index=False)\n",
    "print('test inference is done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "verbal-sample",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
